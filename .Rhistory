p <- par[3]
loglik <- sum(log(p * dpois(x = x, lambda = lambda_1) + (1-p)*dpois(x = x, lambda = lambda_2)))
return(loglik)
}
#If we maximise loglikelihood, we will be minimising AIC
mle_poismix <-
optim(
par = c(25,10,0.5),
fn = calc_poissmix_loglik ,
control = list(fnscale = -1), #maximise
lower = c(0.1,0.1,0),
upper = c(100,100,1) ,
method = "L-BFGS-B"
)
lambda_1_estimate <- mle_poismix$par[1]
lambda_2_estimate <- mle_poismix$par[2]
p_estimate <- mle_poismix$par[3]
aic_poismix <- -2*-mle_poismix$value + 2*(3)
data_frame(`Lamda 1 estimate`= lambda_1_estimate , `Lambda 2 estimate` = lambda_2_estimate, `Mixing Parameter p` = p_estimate, AIC = aic_poismix) %>% knitr::kable()
calc_zip_loglik <- function(par){
lambda <- par[1]
pi <- par[2]
#this for loop is slightly inefficient but it allows us to see what is happening
loglik <- 0
for(j in x){
indicator <- as.numeric(j > 0) #assign value 1 if greater than 0
loglik <- loglik + log( (1 - indicator)*(pi + (1-pi)*exp(-lambda)) + indicator*((1-pi) * dpois(x = j , lambda = lambda))   )
}
return(loglik)
}
#If we maximise loglikelihood, we will be minimising AIC
mle_zip <-
optim(
par = c(10,0.2),
fn = calc_zip_loglik ,
control = list(fnscale = -1), #maximise
lower = c(0.1,0),
upper = c(100,0.99999) ,
method = "L-BFGS-B"
)
lambda_zip_estimate <- mle_zip$par[1]
pi_zip_estimate <- mle_zip$par[2]
aic_zip <- -2*-mle_zip$value + 2*(2)
data_frame(`Lamda estimate`= lambda_zip_estimate , `Pi estimate` = pi_zip_estimate,  AIC = aic_zip) %>% knitr::kable()
aics <- c(aic_poisson,aic_negbinom,aic_poismix,aic_zip)
model <- c("Poisson","Negative Binomial","Poisson Mixture","Zero Inflated Poisson")
data_frame(Model = model, AIC = aics) %>% knitr::kable()
#overlay distributions
#generate zip_predictions
generate_zip_prob <- function(lambda,pi,x){
if(x == 0){
return(pi + (1-pi)*exp(-lambda))
}else{
return((1-pi)*dpois(x=x,lambda=lambda))
}
}
probs <- numeric(81)
for (i in 0:80){
probs[i+1] <- generate_zip_prob(lambda = lambda_zip_estimate ,pi = pi_zip_estimate , x=i)
}
#get expected frequencies
expected_freq <- probs*n
comparison_df <- table(dat$Counts) %>%
as_data_frame %>%
rename(accident_count = Var1,
observed = n) %>%
mutate(expected = expected_freq[1:56],
accident_count = as.numeric(accident_count)) %>%  #take only first 56 values
gather(Type,`Frequency` ,c(observed,expected))
ggplot(comparison_df, aes(x = accident_count , y = Frequency , fill = Type )) +
geom_bar( stat="identity",alpha = .8,
position="dodge") +
labs(x="Accident counts", y="Frequency" , title="Observed vs Expected Frequencies as per Zero Inflated Poisson") +
theme_classic() +
theme(plot.title = element_text(hjust=0.5))
#qq plot
install.packages("fitdistrplus")
library(fitdistrplus)
?fitdist
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, fig.width = 6, fig.height = 5, fig.pos="H", fig.pos = 'H')
# Note: Include = FALSE implies the code is executed, but not printed in your pdf.
# warning and message = FALSE implies ugly messages and warnings are removed from your pdf. These should be picked up when you execute the command chunks (code sections below) in your rmd, not printed in your paper!
library(knitr)
library(ggplot2)
library(kableExtra)
library(tidyverse)
library(cowplot)
library(ZIM) #for generating observations
library(fitdistrplus)
#
# devtools::install_version("rmarkdown", version = "1.8", repos = "http://cran.us.r-project.org")
dat <- read.csv("data/accidents.csv") %>%
setNames(c("Index","Counts")) %>%
as_data_frame() %>%
mutate(Counts = as.numeric(Counts))
#define usefule variables
x <- dat$Counts
n <- nrow(dat)
fit_nbinom <- fitdist(data = x , method = "mle","nbinom")
fit_nbinom
mean(x)
fit_nbinom$aic
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, fig.width = 6, fig.height = 5, fig.pos="H", fig.pos = 'H')
# Note: Include = FALSE implies the code is executed, but not printed in your pdf.
# warning and message = FALSE implies ugly messages and warnings are removed from your pdf. These should be picked up when you execute the command chunks (code sections below) in your rmd, not printed in your paper!
library(knitr)
library(ggplot2)
library(kableExtra)
library(tidyverse)
library(cowplot)
library(ZIM) #for generating observations
library(fitdistrplus)
#
# devtools::install_version("rmarkdown", version = "1.8", repos = "http://cran.us.r-project.org")
dat <- read.csv("data/accidents.csv") %>%
setNames(c("Index","Counts")) %>%
as_data_frame() %>%
mutate(Counts = as.numeric(Counts))
#define usefule variables
x <- dat$Counts
n <- nrow(dat)
#first column is just an index, second is a  1632 observations
#first visualise the data
distribution_plot <- ggplot(dat, aes(Counts)) +
stat_count( fill="blue",
alpha = .4,
position="dodge")+
labs(x="Accident counts", y="Frequency" , title="Histogram\n Accident counts") +
theme_classic() +
theme(plot.title = element_text(hjust=0.5))
firstbox <- ggplot(dat)+
geom_boxplot(aes(y= dat$Counts,x=1),
fill = "cornflowerblue",
outlier.color ="firebrick")+
theme_classic() +
theme(plot.title = element_text(hjust=0.5))+
labs(title="Boxplot\n Accident counts") +
labs(y="Accident counts",x ="")+
coord_flip()
plot_grid(distribution_plot,firstbox,ncol = 1)
percentage_zero <- ((sum(dat$Counts==0)/nrow(dat))*100) %>% round(digits = 2)
summarize(.data = dat, Mean= mean(Counts), Variance = var(Counts), Median = median(Counts)) %>% knitr::kable()
#five number summary of the data
five_sum <- summary(dat)
#percentage outliers - 3rd quartile is 9
perc_outlier <- round(length(dat$Counts[dat$Counts > 1.5*9])/length(dat$Counts),4)*100
#we can use mle to fit this
mle_lambda <- sum(x)/n
loglik_poisson <- -n*mle_lambda + sum(x)*log(mle_lambda)  -sum(log(factorial(x)))
aic_poisson <- -2*loglik_poisson + 2*(1)
data_frame(`Lambda Estimate`= mle_lambda , AIC = aic_poisson) %>% knitr::kable()
#https://en.wikipedia.org/wiki/Negative_binomial_distribution#Gamma%E2%80%93Poisson_mixture
#use optim to optimise for r and m
#r is a shape parameter so it doesnt really affect the loglikelihood much
# we want mean = variance i think... therefore you can solve for r = m^2 /(sample variance  - m)
#This is done by looking at birgit's notes and seeing that variance = m + m^2/r
calc_negbinom_loglik <- function(par){
m <- par[1]
r <- m^2 /(var(x)-m)
loglik <- -n*log(gamma(r)) + sum(log(gamma(r+x))) - sum(log(factorial(x))) + sum(x) * log(m/(m+r)) + n*r*log(r/(r+m))
return(loglik)
}
#If we maximise loglikelihood, we will be minimising AIC
mle_negbinom <-
optim(
par = 1,
fn = calc_negbinom_loglik ,
control = list(fnscale = -1),
lower = 3,
upper = 30 ,
method = "L-BFGS-B"
)
m_estimate <- mle_negbinom$par[1]
r_estimate <- m_estimate^2 /(var(x)-m_estimate)
aic_negbinom <- -2*mle_negbinom$value + 2*(2)
data_frame(`M Mean Estimate`= m_estimate , `r Shape Estimate` = r_estimate, AIC = aic_negbinom) %>% knitr::kable()
calc_poissmix_loglik <- function(par){
lambda_1 <- par[1]
lambda_2 <- par[2]
p <- par[3]
loglik <- sum(log(p * dpois(x = x, lambda = lambda_1) + (1-p)*dpois(x = x, lambda = lambda_2)))
return(loglik)
}
#If we maximise loglikelihood, we will be minimising AIC
mle_poismix <-
optim(
par = c(25,10,0.5),
fn = calc_poissmix_loglik ,
control = list(fnscale = -1), #maximise
lower = c(0.1,0.1,0),
upper = c(100,100,1) ,
method = "L-BFGS-B"
)
lambda_1_estimate <- mle_poismix$par[1]
lambda_2_estimate <- mle_poismix$par[2]
p_estimate <- mle_poismix$par[3]
aic_poismix <- -2*mle_poismix$value + 2*(3)
data_frame(`Lamda 1 estimate`= lambda_1_estimate , `Lambda 2 estimate` = lambda_2_estimate, `Mixing Parameter p` = p_estimate, AIC = aic_poismix) %>% knitr::kable()
calc_zip_loglik <- function(par){
lambda <- par[1]
pi <- par[2]
#this for loop is slightly inefficient but it allows us to see what is happening
loglik <- 0
for(j in x){
indicator <- as.numeric(j > 0) #assign value 1 if greater than 0
loglik <- loglik + log( (1 - indicator)*(pi + (1-pi)*exp(-lambda)) + indicator*((1-pi) * dpois(x = j , lambda = lambda))   )
}
return(loglik)
}
#If we maximise loglikelihood, we will be minimising AIC
mle_zip <-
optim(
par = c(10,0.2),
fn = calc_zip_loglik ,
control = list(fnscale = -1), #maximise
lower = c(0.1,0),
upper = c(100,0.99999) ,
method = "L-BFGS-B"
)
lambda_zip_estimate <- mle_zip$par[1]
pi_zip_estimate <- mle_zip$par[2]
aic_zip <- -2*mle_zip$value + 2*(2)
data_frame(`Lamda estimate`= lambda_zip_estimate , `Pi estimate` = pi_zip_estimate,  AIC = aic_zip) %>% knitr::kable()
aics <- c(aic_poisson,aic_negbinom,aic_poismix,aic_zip)
model <- c("Poisson","Negative Binomial","Poisson Mixture","Zero Inflated Poisson")
data_frame(Model = model, AIC = aics) %>% knitr::kable()
#overlay distributions
#generate zip_predictions
generate_zip_prob <- function(lambda,pi,x){
if(x == 0){
return(pi + (1-pi)*exp(-lambda))
}else{
return((1-pi)*dpois(x=x,lambda=lambda))
}
}
probs <- numeric(81)
for (i in 0:80){
probs[i+1] <- generate_zip_prob(lambda = lambda_zip_estimate ,pi = pi_zip_estimate , x=i)
}
#get expected frequencies
expected_freq <- probs*n
comparison_df <- table(dat$Counts) %>%
as_data_frame %>%
rename(accident_count = Var1,
observed = n) %>%
mutate(expected = expected_freq[1:56],
accident_count = as.numeric(accident_count)) %>%  #take only first 56 values
gather(Type,`Frequency` ,c(observed,expected))
ggplot(comparison_df, aes(x = accident_count , y = Frequency , fill = Type )) +
geom_bar( stat="identity",alpha = .8,
position="dodge") +
labs(x="Accident counts", y="Frequency" , title="Observed vs Expected Frequencies as per Zero Inflated Poisson") +
theme_classic() +
theme(plot.title = element_text(hjust=0.5))
#qq plot
#check if any of this is right
fit_nbinom <- fitdist(data = x , method = "mle","nbinom")
aic_negbinom
generate_negbinom_prob <- function(m,r,x){
(gamma(r + x))/(factorial(x)*gamma(r)) * (m/(m+r))^(x) * (r /(r+m))^(r)
}
generate_negbinom_prob <- function(m,r,x){
(gamma(r + x))/(factorial(x)*gamma(r)) * (m/(m+r))^(x) * (r /(r+m))^(r)
}
probs <- numeric(81)
for (i in 0:80){
probs[i+1] <- generate_negbinom_prob(m = m_estimate , r = r_estimate , x=i)
}
#get expected frequencies
expected_freq <- probs*n
comparison_df <- table(dat$Counts) %>%
as_data_frame %>%
rename(accident_count = Var1,
observed = n) %>%
mutate(expected = expected_freq[1:56],
accident_count = as.numeric(accident_count)) %>%  #take only first 56 values
gather(Type,`Frequency` ,c(observed,expected))
ggplot(comparison_df, aes(x = accident_count , y = Frequency , fill = Type )) +
geom_bar( stat="identity",alpha = .8,
position="dodge") +
labs(x="Accident counts", y="Frequency" , title="Observed vs Expected Frequencies as per Zero Inflated Poisson") +
theme_classic() +
theme(plot.title = element_text(hjust=0.5))
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, fig.width = 6, fig.height = 5, fig.pos="H", fig.pos = 'H')
# Note: Include = FALSE implies the code is executed, but not printed in your pdf.
# warning and message = FALSE implies ugly messages and warnings are removed from your pdf. These should be picked up when you execute the command chunks (code sections below) in your rmd, not printed in your paper!
library(knitr)
library(ggplot2)
library(kableExtra)
library(tidyverse)
library(cowplot)
library(ZIM) #for generating observations
library(fitdistrplus)
#
# devtools::install_version("rmarkdown", version = "1.8", repos = "http://cran.us.r-project.org")
dat <- read.csv("data/accidents.csv") %>%
setNames(c("Index","Counts")) %>%
as_data_frame() %>%
mutate(Counts = as.numeric(Counts))
#define usefule variables
x <- dat$Counts
n <- nrow(dat)
#first column is just an index, second is a  1632 observations
#first visualise the data
distribution_plot <- ggplot(dat, aes(Counts)) +
stat_count( fill="blue",
alpha = .4,
position="dodge")+
labs(x="Accident counts", y="Frequency" , title="Histogram\n Accident counts") +
theme_classic() +
theme(plot.title = element_text(hjust=0.5))
firstbox <- ggplot(dat)+
geom_boxplot(aes(y= dat$Counts,x=1),
fill = "cornflowerblue",
outlier.color ="firebrick")+
theme_classic() +
theme(plot.title = element_text(hjust=0.5))+
labs(title="Boxplot\n Accident counts") +
labs(y="Accident counts",x ="")+
coord_flip()
plot_grid(distribution_plot,firstbox,ncol = 1)
percentage_zero <- ((sum(dat$Counts==0)/nrow(dat))*100) %>% round(digits = 2)
summarize(.data = dat, Mean= mean(Counts), Variance = var(Counts), Median = median(Counts)) %>% knitr::kable()
#five number summary of the data
five_sum <- summary(dat)
#percentage outliers - 3rd quartile is 9
perc_outlier <- round(length(dat$Counts[dat$Counts > 1.5*9])/length(dat$Counts),4)*100
#we can use mle to fit this
mle_lambda <- sum(x)/n
loglik_poisson <- -n*mle_lambda + sum(x)*log(mle_lambda)  -sum(log(factorial(x)))
aic_poisson <- -2*loglik_poisson + 2*(1)
data_frame(`Lambda Estimate`= mle_lambda , AIC = aic_poisson) %>% knitr::kable()
#https://en.wikipedia.org/wiki/Negative_binomial_distribution#Gamma%E2%80%93Poisson_mixture
#use optim to optimise for r and m
#r is a shape parameter so it doesnt really affect the loglikelihood much
# we want mean = variance i think... therefore you can solve for r = m^2 /(sample variance  - m)
#This is done by looking at birgit's notes and seeing that variance = m + m^2/r
calc_negbinom_loglik <- function(par){
m <- par[1]
r <- m^2 /(var(x)-m)
loglik <- -n*log(gamma(r)) + sum(log(gamma(r+x))) - sum(log(factorial(x))) + sum(x) * log(m/(m+r)) + n*r*log(r/(r+m))
return(loglik)
}
#If we maximise loglikelihood, we will be minimising AIC
mle_negbinom <-
optim(
par = 1,
fn = calc_negbinom_loglik ,
control = list(fnscale = -1),
lower = 3,
upper = 30 ,
method = "L-BFGS-B"
)
m_estimate <- mle_negbinom$par[1]
r_estimate <- m_estimate^2 /(var(x)-m_estimate)
aic_negbinom <- -2*mle_negbinom$value + 2*(2)
data_frame(`M Mean Estimate`= m_estimate , `r Shape Estimate` = r_estimate, AIC = aic_negbinom) %>% knitr::kable()
calc_poissmix_loglik <- function(par){
lambda_1 <- par[1]
lambda_2 <- par[2]
p <- par[3]
loglik <- sum(log(p * dpois(x = x, lambda = lambda_1) + (1-p)*dpois(x = x, lambda = lambda_2)))
return(loglik)
}
#If we maximise loglikelihood, we will be minimising AIC
mle_poismix <-
optim(
par = c(25,10,0.5),
fn = calc_poissmix_loglik ,
control = list(fnscale = -1), #maximise
lower = c(0.1,0.1,0),
upper = c(100,100,1) ,
method = "L-BFGS-B"
)
lambda_1_estimate <- mle_poismix$par[1]
lambda_2_estimate <- mle_poismix$par[2]
p_estimate <- mle_poismix$par[3]
aic_poismix <- -2*mle_poismix$value + 2*(3)
data_frame(`Lamda 1 estimate`= lambda_1_estimate , `Lambda 2 estimate` = lambda_2_estimate, `Mixing Parameter p` = p_estimate, AIC = aic_poismix) %>% knitr::kable()
calc_zip_loglik <- function(par){
lambda <- par[1]
pi <- par[2]
#this for loop is slightly inefficient but it allows us to see what is happening
loglik <- 0
for(j in x){
indicator <- as.numeric(j > 0) #assign value 1 if greater than 0
loglik <- loglik + log( (1 - indicator)*(pi + (1-pi)*exp(-lambda)) + indicator*((1-pi) * dpois(x = j , lambda = lambda))   )
}
return(loglik)
}
#If we maximise loglikelihood, we will be minimising AIC
mle_zip <-
optim(
par = c(10,0.2),
fn = calc_zip_loglik ,
control = list(fnscale = -1), #maximise
lower = c(0.1,0),
upper = c(100,0.99999) ,
method = "L-BFGS-B"
)
lambda_zip_estimate <- mle_zip$par[1]
pi_zip_estimate <- mle_zip$par[2]
aic_zip <- -2*mle_zip$value + 2*(2)
data_frame(`Lamda estimate`= lambda_zip_estimate , `Pi estimate` = pi_zip_estimate,  AIC = aic_zip) %>% knitr::kable()
aics <- c(aic_poisson,aic_negbinom,aic_poismix,aic_zip)
model <- c("Poisson","Negative Binomial","Poisson Mixture","Zero Inflated Poisson")
data_frame(Model = model, AIC = aics) %>% knitr::kable()
#overlay distributions
#generate zip_predictions
# generate_zip_prob <- function(lambda,pi,x){
#
#   if(x == 0){
#     return(pi + (1-pi)*exp(-lambda))
#   }else{
#     return((1-pi)*dpois(x=x,lambda=lambda))
#
#   }
#
# }
generate_negbinom_prob <- function(m,r,x){
(gamma(r + x))/(factorial(x)*gamma(r)) * (m/(m+r))^(x) * (r /(r+m))^(r)
}
probs <- numeric(81)
for (i in 0:80){
probs[i+1] <- generate_negbinom_prob(m = m_estimate , r = r_estimate , x=i)
}
#get expected frequencies
expected_freq <- probs*n
comparison_df <- table(dat$Counts) %>%
as_data_frame %>%
rename(accident_count = Var1,
observed = n) %>%
mutate(expected = expected_freq[1:56],
accident_count = as.numeric(accident_count)) %>%  #take only first 56 values
gather(Type,`Frequency` ,c(observed,expected))
ggplot(comparison_df, aes(x = accident_count , y = Frequency , fill = Type )) +
geom_bar( stat="identity",alpha = .8,
position="dodge") +
labs(x="Accident counts", y="Frequency" , title="Observed vs Expected Frequencies as per Negative Binomial") +
theme_classic() +
theme(plot.title = element_text(hjust=0.5))
#qq plot
#check if any of this is right
fit_nbinom <- fitdist(data = x , method = "mle","nbinom")
fit_nbinom$aic
plot(fit_nbinom)
fitdistrplus::qqcomp(fit_nbinom)
?qqcomp
fitdistrplus::qqcomp(fit_nbinom,addlegend = F, main = "Q-Q plot for Negative Binomial")
fitdistrplus::qqcomp(fit_nbinom,addlegend = F, main = "Q-Q plot for Negative Binomial")
str(comparison_df)
filter(comparison_df,Type == "observed")$Frequency
chisq.test(filter(comparison_df,Type == "observed")$Frequency,filter(comparison_df,Type != "observed")$Frequency)
calc_negbinom_loglik
m=6
calc_negbinom_loglik_profile <- function(r,m){
loglik <- -n*log(gamma(r)) + sum(log(gamma(r+x))) - sum(log(factorial(x))) + sum(x) * log(m/(m+r)) + n*r*log(r/(r+m))
return(loglik)
}
mle_profile <-
optim(
par = 1,
m = m,
fn = calc_negbinom_loglik_profile ,
control = list(fnscale = -1),
lower = 0,
upper = 30 ,
method = "L-BFGS-B"
)
mle_profile <-
optim(
par = 1,
m = m,
fn = calc_negbinom_loglik_profile ,
control = list(fnscale = -1),
lower = 0.1,
upper = 30 ,
method = "L-BFGS-B"
)
mle_profile$par
calc_negbinom_loglik_profile <- function(r,m){
loglik <- -n*log(gamma(r)) + sum(log(gamma(r+x))) - sum(log(factorial(x))) + sum(x) * log(m/(m+r)) + n*r*log(r/(r+m))
return(loglik)
}
profile_likelihood_calc_mean <- function(){
#Calculate profile for M by calculating mle for R at eacg value of M
estimated_r_s <- numeric(56)
for(m in 1:56){
mle_profile <-
optim(
par = 1,
m = m,
fn = calc_negbinom_loglik_profile ,
control = list(fnscale = -1),
lower = 0.1,
upper = 30 ,
method = "L-BFGS-B"
)
estimated_r_s[m] <- mle_profile$par
}
return(estimated_r_s)
}
estimated_r_values <- profile_likelihood_calc_mean()
estimated_r_values
m_profile_loglik_values <- numeric(56)
for(i in seq_along(estimated_r_values)){print(i)}
for (i in seq_along(estimated_r_values)){
m_profile_loglik_values[i] <- calc_negbinom_loglik_profile(r = estimated_r_values[i], m = i)
}
m_profile_loglik_values
plot(x = 1:56 , y = m_profile_loglik_values , type = "l")
plot(x = 1:56 , y = m_profile_loglik_values , type = "l", xlab = "M parameter value", ylab = "Profile Loglikelihood")
