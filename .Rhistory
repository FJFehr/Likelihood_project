#have to redfine function to allow for correct optimisation
calc_negbinom_loglik_profile <- function(m,r){
loglik <- -n*log(gamma(r)) + sum(log(gamma(r+x))) - sum(log(factorial(x))) + sum(x) * log(m/(m+r)) + n*r*log(r/(r+m))
return(loglik)
}
profile_likelihood_calc_shape <- function(){
#Calculate profile for R by calculating mle for M at eacg value of R
estimated_m_s <- numeric(length(r_range))
for(i in seq_along(r_range)){
mle_profile <- #find mle for r at each value of m
optim(
par = 1,
r = r_range[i],
fn = calc_negbinom_loglik_profile ,
control = list(fnscale = -1),
lower = 1,
upper = 100 ,
method = "L-BFGS-B"
)
estimated_m_s[i] <- mle_profile$par
}
return(estimated_m_s)
}
estimated_m_values <- profile_likelihood_calc_shape() #all m values are the same, this make sense as m is the dude who will afect loglikelihood
r_profile_loglik_values <- numeric(length(r_range))
for (i in seq_along(estimated_m_values)){
r_profile_loglik_values[i] <- calc_negbinom_loglik_profile(m = estimated_m_values[i], r = r_range[i])
}
data_frame(x = r_range, val = r_profile_loglik_values) %>%
ggplot(aes(x=x,y=val))+
geom_point(size = 1) +
scale_x_continuous(breaks = seq(0,0.8,by=0.05)) +
geom_vline(xintercept = 0.58, col = "red", lty = "dashed") +
labs(x = "R shape parameter value", y = "Profile Loglikelihood" , title = "Profile Loglikelihood of R Shape Parameter") +
theme_classic()
#https://en.wikipedia.org/wiki/Negative_binomial_distribution#Gamma%E2%80%93Poisson_mixture
#use optim to optimise for r and m
#r is a shape parameter so it doesnt really affect the loglikelihood much
# we want mean = variance i think... therefore you can solve for r = m^2 /(sample variance  - m)
#This is done by looking at birgit's notes and seeing that variance = m + m^2/r
calc_negbinom_loglik <- function(par){
m <- par[1]
r <- m^2 /(var(x)-m)
loglik <- -n*log(gamma(r)) + sum(log(gamma(r+x))) - sum(log(factorial(x))) + sum(x) * log(m/(m+r)) + n*r*log(r/(r+m))
return(loglik)
}
#If we maximise loglikelihood, we will be minimising AIC
mle_negbinom <-
optim(
par = 1,
fn = calc_negbinom_loglik ,
control = list(fnscale = -1),
lower = 3,
upper = 30 ,
method = "L-BFGS-B"
)
m_estimate <- mle_negbinom$par[1]
r_estimate <- m_estimate^2 /(var(x)-m_estimate)
aic_negbinom <- -2*mle_negbinom$value + 2*(2)
data_frame(`M Mean Estimate`= m_estimate , `r Shape Estimate` = r_estimate, AIC = aic_negbinom) %>% knitr::kable()
#https://en.wikipedia.org/wiki/Negative_binomial_distribution#Gamma%E2%80%93Poisson_mixture
#use optim to optimise for r and m
#r is a shape parameter so it doesnt really affect the loglikelihood much
# we want mean = variance i think... therefore you can solve for r = m^2 /(sample variance  - m)
#This is done by looking at birgit's notes and seeing that variance = m + m^2/r
calc_negbinom_loglik <- function(par){
m <- par[1]
r <- m^2 /(var(x)-m)
loglik <- -n*log(gamma(r)) + sum(log(gamma(r+x))) - sum(log(factorial(x))) + sum(x) * log(m/(m+r)) + n*r*log(r/(r+m))
return(loglik)
}
#If we maximise loglikelihood, we will be minimising AIC
mle_negbinom <-
optim(
par = 1,
fn = calc_negbinom_loglik ,
control = list(fnscale = -1),
lower = 3,
upper = 30 ,
method = "L-BFGS-B"
)
m_estimate <- mle_negbinom$par[1]
r_estimate <- m_estimate^2 /(var(x)-m_estimate)
aic_negbinom <- -2*mle_negbinom$value + 2*(2)
data_frame(`M Mean Estimate`= m_estimate , `r Shape Estimate` = r_estimate, AIC = aic_negbinom) %>% knitr::kable()
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, fig.width = 6, fig.height = 5, fig.pos="H", fig.pos = 'H')
# Note: Include = FALSE implies the code is executed, but not printed in your pdf.
# warning and message = FALSE implies ugly messages and warnings are removed from your pdf. These should be picked up when you execute the command chunks (code sections below) in your rmd, not printed in your paper!
library(knitr)
library(ggplot2)
library(kableExtra)
library(tidyverse)
library(cowplot)
library(ZIM) #for generating observations
library(fitdistrplus)
#
# devtools::install_version("rmarkdown", version = "1.8", repos = "http://cran.us.r-project.org")
dat <- read.csv("data/accidents.csv") %>%
setNames(c("Index","Counts")) %>%
as_data_frame() %>%
mutate(Counts = as.numeric(Counts))
#define usefule variables
x <- dat$Counts
n <- nrow(dat)
#first column is just an index, second is a  1632 observations
#first visualise the data
distribution_plot <- ggplot(dat, aes(Counts)) +
stat_count( fill="blue",
alpha = .4,
position="dodge")+
labs(x="Accident counts", y="Frequency" , title="Histogram\n Accident counts") +
theme_classic() +
theme(plot.title = element_text(hjust=0.5))
firstbox <- ggplot(dat)+
geom_boxplot(aes(y= dat$Counts,x=1),
fill = "cornflowerblue",
outlier.color ="firebrick")+
theme_classic() +
theme(plot.title = element_text(hjust=0.5))+
labs(title="Boxplot\n Accident counts") +
labs(y="Accident counts",x ="")+
coord_flip()
plot_grid(distribution_plot,firstbox,ncol = 1)
percentage_zero <- ((sum(dat$Counts==0)/nrow(dat))*100) %>% round(digits = 2)
summarize(.data = dat, Mean= mean(Counts), Variance = var(Counts), Median = median(Counts)) %>% knitr::kable()
#five number summary of the data
five_sum <- summary(dat)
#percentage outliers - 3rd quartile is 9
perc_outlier <- round(length(dat$Counts[dat$Counts > 1.5*9])/length(dat$Counts),4)*100
#we can use mle to fit this
mle_lambda <- sum(x)/n
loglik_poisson <- -n*mle_lambda + sum(x)*log(mle_lambda)  -sum(log(factorial(x)))
aic_poisson <- -2*loglik_poisson + 2*(1)
data_frame(`Lambda Estimate`= mle_lambda , AIC = aic_poisson) %>% knitr::kable()
#https://en.wikipedia.org/wiki/Negative_binomial_distribution#Gamma%E2%80%93Poisson_mixture
#use optim to optimise for r and m
#r is a shape parameter so it doesnt really affect the loglikelihood much
# we want mean = variance i think... therefore you can solve for r = m^2 /(sample variance  - m)
#This is done by looking at birgit's notes and seeing that variance = m + m^2/r
calc_negbinom_loglik <- function(par){
m <- par[1]
r <- m^2 /(var(x)-m)
loglik <- -n*log(gamma(r)) + sum(log(gamma(r+x))) - sum(log(factorial(x))) + sum(x) * log(m/(m+r)) + n*r*log(r/(r+m))
return(loglik)
}
#If we maximise loglikelihood, we will be minimising AIC
mle_negbinom <-
optim(
par = 1,
fn = calc_negbinom_loglik ,
control = list(fnscale = -1),
lower = 3,
upper = 30 ,
method = "L-BFGS-B"
)
m_estimate <- mle_negbinom$par[1]
r_estimate <- m_estimate^2 /(var(x)-m_estimate)
aic_negbinom <- -2*mle_negbinom$value + 2*(2)
data_frame(`M Mean Estimate`= m_estimate , `r Shape Estimate` = r_estimate, AIC = aic_negbinom) %>% knitr::kable()
calc_poissmix_loglik <- function(par){
lambda_1 <- par[1]
lambda_2 <- par[2]
p <- par[3]
loglik <- sum(log(p * dpois(x = x, lambda = lambda_1) + (1-p)*dpois(x = x, lambda = lambda_2)))
return(loglik)
}
#If we maximise loglikelihood, we will be minimising AIC
mle_poismix <-
optim(
par = c(25,10,0.5),
fn = calc_poissmix_loglik ,
control = list(fnscale = -1), #maximise
lower = c(0.1,0.1,0),
upper = c(100,100,1) ,
method = "L-BFGS-B"
)
lambda_1_estimate <- mle_poismix$par[1]
lambda_2_estimate <- mle_poismix$par[2]
p_estimate <- mle_poismix$par[3]
aic_poismix <- -2*mle_poismix$value + 2*(3)
data_frame(`Lamda 1 estimate`= lambda_1_estimate , `Lambda 2 estimate` = lambda_2_estimate, `Mixing Parameter p` = p_estimate, AIC = aic_poismix) %>% knitr::kable()
calc_zip_loglik <- function(par){
lambda <- par[1]
pi <- par[2]
#this for loop is slightly inefficient but it allows us to see what is happening
loglik <- 0
for(j in x){
indicator <- as.numeric(j > 0) #assign value 1 if greater than 0
loglik <- loglik + log( (1 - indicator)*(pi + (1-pi)*exp(-lambda)) + indicator*((1-pi) * dpois(x = j , lambda = lambda))   )
}
return(loglik)
}
#If we maximise loglikelihood, we will be minimising AIC
mle_zip <-
optim(
par = c(10,0.2),
fn = calc_zip_loglik ,
control = list(fnscale = -1), #maximise
lower = c(0.1,0),
upper = c(100,0.99999) ,
method = "L-BFGS-B"
)
lambda_zip_estimate <- mle_zip$par[1]
pi_zip_estimate <- mle_zip$par[2]
aic_zip <- -2*mle_zip$value + 2*(2)
data_frame(`Lamda estimate`= lambda_zip_estimate , `Pi estimate` = pi_zip_estimate,  AIC = aic_zip) %>% knitr::kable()
aics <- c(aic_poisson,aic_negbinom,aic_poismix,aic_zip)
model <- c("Poisson","Negative Binomial","Poisson Mixture","Zero Inflated Poisson")
data_frame(Model = model, AIC = aics) %>% knitr::kable()
#overlay distributions
#generate zip_predictions
# generate_zip_prob <- function(lambda,pi,x){
#
#   if(x == 0){
#     return(pi + (1-pi)*exp(-lambda))
#   }else{
#     return((1-pi)*dpois(x=x,lambda=lambda))
#
#   }
#
# }
generate_negbinom_prob <- function(m,r,x){
(gamma(r + x))/(factorial(x)*gamma(r)) * (m/(m+r))^(x) * (r /(r+m))^(r)
}
probs <- numeric(81)
for (i in 0:80){
probs[i+1] <- generate_negbinom_prob(m = m_estimate , r = r_estimate , x=i)
}
#get expected frequencies
expected_freq <- probs*n
comparison_df <- table(dat$Counts) %>%
as_data_frame %>%
rename(accident_count = Var1,
observed = n) %>%
mutate(expected = expected_freq[1:56],
accident_count = as.numeric(accident_count)) %>%  #take only first 56 values
gather(Type,`Frequency` ,c(observed,expected))
ggplot(comparison_df, aes(x = accident_count , y = Frequency , fill = Type )) +
geom_bar( stat="identity",alpha = .8,
position="dodge") +
labs(x="Accident counts", y="Frequency" , title="Observed vs Expected Frequencies as per Negative Binomial") +
theme_classic() +
theme(plot.title = element_text(hjust=0.5))
#qq plot
#check if any of this is right
fit_nbinom <- fitdist(data = x , method = "mle","nbinom")
# q-q
fitdistrplus::qqcomp(fit_nbinom,addlegend = F, main = "Q-Q plot for Negative Binomial")
chisq.test(filter(comparison_df,Type == "observed")$Frequency,filter(comparison_df,Type != "observed")$Frequency)
#need to calulcate mle at EACH point
m_range <- seq(1,56, by = 0.1) #values of m for profile m loglikelihood
r_range <- seq(0.01,0.8 , by = 0.01)
calc_negbinom_loglik_profile <- function(r,m){
loglik <- -n*log(gamma(r)) + sum(log(gamma(r+x))) - sum(log(factorial(x))) + sum(x) * log(m/(m+r)) + n*r*log(r/(r+m))
return(loglik)
}
profile_likelihood_calc_mean <- function(){
#Calculate profile for M by calculating mle for R at eacg value of M
estimated_r_s <- numeric(length(m_range))
for(i in seq_along(m_range)){
mle_profile <- #find mle for r at each value of m
optim(
par = 1,
m = m_range[i],
fn = calc_negbinom_loglik_profile ,
control = list(fnscale = -1),
lower = 0.1,
upper = 30 ,  # i presume shape cant take a ridiculous value like this
method = "L-BFGS-B"
)
estimated_r_s[i] <- mle_profile$par
}
return(estimated_r_s)
}
estimated_r_values <- profile_likelihood_calc_mean()
#For each r in estimated_r_values, sub in M from 1:56 and trace it
m_profile_loglik_values <- numeric(length(m_range))
for (i in seq_along(estimated_r_values)){
m_profile_loglik_values[i] <- calc_negbinom_loglik_profile(r = estimated_r_values[i], m = m_range[i])
}
data_frame(x = m_range, val = m_profile_loglik_values) %>%
ggplot(aes(x=x,y=val))+
geom_point(size = 1) +
scale_x_continuous(breaks = seq(0,56,by=2)) +
geom_vline(xintercept = 6.9, col = "red", lty = "dashed") +
labs(x = "M parameter value", y = "Profile Loglikelihood" , title = "Profile Loglikelihood of M Parameter") +
theme_classic()
#Now we do the profile loglikelihood for the r
#have to redfine function to allow for correct optimisation
calc_negbinom_loglik_profile <- function(m,r){
loglik <- -n*log(gamma(r)) + sum(log(gamma(r+x))) - sum(log(factorial(x))) + sum(x) * log(m/(m+r)) + n*r*log(r/(r+m))
return(loglik)
}
profile_likelihood_calc_shape <- function(){
#Calculate profile for R by calculating mle for M at eacg value of R
estimated_m_s <- numeric(length(r_range))
for(i in seq_along(r_range)){
mle_profile <- #find mle for r at each value of m
optim(
par = 1,
r = r_range[i],
fn = calc_negbinom_loglik_profile ,
control = list(fnscale = -1),
lower = 1,
upper = 100 ,
method = "L-BFGS-B"
)
estimated_m_s[i] <- mle_profile$par
}
return(estimated_m_s)
}
estimated_m_values <- profile_likelihood_calc_shape() #all m values are the same, this make sense as m is the dude who will afect loglikelihood
r_profile_loglik_values <- numeric(length(r_range))
for (i in seq_along(estimated_m_values)){
r_profile_loglik_values[i] <- calc_negbinom_loglik_profile(m = estimated_m_values[i], r = r_range[i])
}
data_frame(x = r_range, val = r_profile_loglik_values) %>%
ggplot(aes(x=x,y=val))+
geom_point(size = 1) +
scale_x_continuous(breaks = seq(0,0.8,by=0.05)) +
geom_vline(xintercept = 0.58, col = "red", lty = "dashed") +
labs(x = "R shape parameter value", y = "Profile Loglikelihood" , title = "Profile Loglikelihood of R Shape Parameter") +
theme_classic()
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, fig.width = 6, fig.height = 5, fig.pos="H", fig.pos = 'H')
# Note: Include = FALSE implies the code is executed, but not printed in your pdf.
# warning and message = FALSE implies ugly messages and warnings are removed from your pdf. These should be picked up when you execute the command chunks (code sections below) in your rmd, not printed in your paper!
library(knitr)
library(ggplot2)
library(kableExtra)
library(tidyverse)
library(cowplot) #for plotting cows
library(ZIM) #for generating observations
library(fitdistrplus)
#
# devtools::install_version("rmarkdown", version = "1.8", repos = "http://cran.us.r-project.org")
dat <- read.csv("data/accidents.csv") %>%
setNames(c("Index","Counts")) %>%
as_data_frame() %>%
mutate(Counts = as.numeric(Counts))
#define usefule variables
x <- dat$Counts
n <- nrow(dat)
fisher_info_m <- function(r,m){
info <- (n*r*m*(m+r)+sum(x)*r-n*m*r*(r+r*m))/(m^2*(r+m)^2)
return(info)
}
fisher_info_m(r_estimate,m_estimate)
#dont you need to invert this?
se_m <- sqrt(fisher_info_m(r_estimate,m_estimate)^(-1))
se_m
max_m_loglikelihood <- calc_negbinom_loglik_profile(m = m_estimate , r = r_estimate)
out <- #using function that i used to calculate joint MLE for m and R the only downside to this is that it is NOT taking in different shape parameters...check out the function
optim(
par = 1,
fn = calc_negbinom_loglik ,
control = list(fnscale = -1),
lower = 3,
upper = 30 ,
method = "L-BFGS-B"
)
max_m_loglikelihood <- calc_negbinom_loglik_profile(m = m_estimate , r = r_estimate)
fnct <- function(m, x , max_m_loglikelihood, cutoff = chisq(.95,1)) {
out <- #using function that i used to calculate joint MLE for m and R the only downside to this is that it is NOT taking in different shape parameters...check out the function
optim(
par = 1,
fn = calc_negbinom_loglik ,
control = list(fnscale = -1),
lower = 3,
upper = 30 ,
method = "L-BFGS-B"
)
f <- 2 * (max_m_loglikelihood - out$objective) - cutoff #this value should be zero for the constraints to be satisfied
#replcae qchisq() with a random gamma cutoff if necesary
}
uniroot(fnct, c(1, m_estimate), x = x , max_m_loglikelihood = max_m_loglikelihood)
max_m_loglikelihood <- calc_negbinom_loglik_profile(m = m_estimate , r = r_estimate)
fnct <- function(m, x , max_m_loglikelihood, cutoff = qchisq(.95,1)) {
out <- #using function that i used to calculate joint MLE for m and R the only downside to this is that it is NOT taking in different shape parameters...check out the function
optim(
par = 1,
fn = calc_negbinom_loglik ,
control = list(fnscale = -1),
lower = 3,
upper = 30 ,
method = "L-BFGS-B"
)
f <- 2 * (max_m_loglikelihood - out$objective) - cutoff #this value should be zero for the constraints to be satisfied
#replcae qchisq() with a random gamma cutoff if necesary
}
uniroot(fnct, c(1, m_estimate), x = x , max_m_loglikelihood = max_m_loglikelihood)
#plug in mle values to get l(theta_hat)
max_m_loglikelihood <- calc_negbinom_loglik_profile(m = m_estimate , r = r_estimate)
fnct <- function(m, max_m_loglikelihood, cutoff = qchisq(.95,1)) {
# out <- #using function that i used to calculate joint MLE for m and R the only downside to this is that it is NOT taking in different shape parameters...check out the function
# optim(
# par = m,
# fn = calc_negbinom_loglik ,
# control = list(fnscale = -1),
# lower = 3,
# upper = 30 ,
# method = "L-BFGS-B"
# )
f <- 2 * (max_m_loglikelihood - calc_negbinom_loglik_profile(m = m ,r =r_estimate)) - cutoff #this value should be zero for the constraints to be satisfied
#replcae qchisq() with a random gamma cutoff if necesary
}
uniroot(fnct, c(1, m_estimate) , max_m_loglikelihood = max_m_loglikelihood)
lower_bound_wilk_interval <- uniroot(fnct, c(1, m_estimate) , max_m_loglikelihood = max_m_loglikelihood)
upper_bound_wilk_interval <- uniroot(fnct, c(m_estimate, 52) , max_m_loglikelihood = max_m_loglikelihood)
upper_bound_wilk_interval
fnct_direct <- function(m, max_m_loglikelihood, gamma = 0.15) {
# out <- #using function that i used to calculate joint MLE for m and R the only downside to this is that it is NOT taking in different shape parameters...check out the function
# optim(
# par = m,
# fn = calc_negbinom_loglik ,
# control = list(fnscale = -1),
# lower = 3,
# upper = 30 ,
# method = "L-BFGS-B"
# )
f <-  (max_m_loglikelihood - calc_negbinom_loglik_profile(m = m ,r =r_estimate)) - log(gamma) #this value should be zero for the constraints to be satisfied
#replcae qchisq() with a random gamma cutoff if necesary
}
lower_bound_direct_interval_m <- uniroot(fnct_direct, c(1, m_estimate) , max_m_loglikelihood = max_m_loglikelihood , gamma = 0.15 )
log(0.15)
wilks_likelihood
wilks_likelihood <- c(lower_bound_wilk_interval$root,upper_bound_wilk_interval$root)
wilks_likelihood
wald_m
wald_m
#according to my maths this would give us the fisher info (sent a picture via whatsapp)
fisher_info_m <- function(r,m){
info <- (n*r*m*(m+r)+sum(x)*r-n*m*r*(r+r*m))/(m^2*(r+m)^2)
return(info)
}
se_m <- sqrt(fisher_info_m(r_estimate,m_estimate)^(-1))
wald_m <- c(0,0)
wald_m[1] <- m_estimate-1.96*se_m
wald_m[2] <- m_estimate+1.96*se_m
wilks_likelihood
wald_m
install.packages("jpeg")
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, fig.width = 6, fig.height = 5, fig.pos="H", fig.pos = 'H')
# Note: Include = FALSE implies the code is executed, but not printed in your pdf.
# warning and message = FALSE implies ugly messages and warnings are removed from your pdf. These should be picked up when you execute the command chunks (code sections below) in your rmd, not printed in your paper!
library(knitr)
library(ggplot2)
library(kableExtra)
library(tidyverse)
library(cowplot) #for plotting cows
library(ZIM) #for generating observations
library(fitdistrplus)
library(jpeg)# for pulling in our likelihood surface
#
# devtools::install_version("rmarkdown", version = "1.8", repos = "http://cran.us.r-project.org")
dat <- read.csv("data/accidents.csv") %>%
setNames(c("Index","Counts")) %>%
as_data_frame() %>%
mutate(Counts = as.numeric(Counts))
#define usefule variables
x <- dat$Counts
n <- nrow(dat)
fnct_direct <- function(r, max_m_loglikelihood, gamma = 0.15) {
m <- m_estimate #mle for M under r is just m_estimate
f <-  (max_m_loglikelihood - calc_negbinom_loglik_profile(m = m ,r =r)) +1*log(gamma) #this value should be zero for the constraints to be satisfied
#replcae qchisq() with a random gamma cutoff if necesary
}
#here is an example of how to do direct likelihood for M
lower_bound_direct_interval_r <- uniroot(fnct_direct, c(0.01, r_estimate) , max_m_loglikelihood = max_m_loglikelihood)
lower_bound_direct_interval_r <- uniroot(fnct_direct, c(r_estimate, 0.6) , max_m_loglikelihood = max_m_loglikelihood)
r_estimate
fnct_direct <- function(r, max_m_loglikelihood, gamma = 0.15) {
m <- m_estimate #mle for M under r is just m_estimate
f <-  (max_m_loglikelihood - calc_negbinom_loglik_profile(m = m ,r =r)) +1*log(gamma) #this value should be zero for the constraints to be satisfied
#replcae qchisq() with a random gamma cutoff if necesary
}
#here is an example of how to do direct likelihood for M
lower_bound_direct_interval_r <- uniroot(fnct_direct, c(0.1, r_estimate) , max_m_loglikelihood = max_m_loglikelihood)
lower_bound_direct_interval_r <- uniroot(fnct_direct, c(r_estimate, 0.9) , max_m_loglikelihood = max_m_loglikelihood)
direct_interval_r <- c(lower_bound_direct_interval_r$root,lower_bound_direct_interval_r$root)
direct_interval_r
r_estimate
fnct_direct_r <- function(r, max_m_loglikelihood, gamma = 0.15) {
m <- m_estimate #mle for M under r is just m_estimate
f <-  (max_m_loglikelihood - calc_negbinom_loglik_profile(m = m ,r =r)) +1*log(gamma) #this value should be zero for the constraints to be satisfied
#replcae qchisq() with a random gamma cutoff if necesary
}
#here is an example of how to do direct likelihood for M
lower_bound_direct_interval_r <- uniroot(fnct_direct_r, c(0.1, r_estimate) , max_m_loglikelihood = max_m_loglikelihood)
lower_bound_direct_interval_r <- uniroot(fnct_direct_r, c(r_estimate, 0.9) , max_m_loglikelihood = max_m_loglikelihood)
direct_interval_r <- c(lower_bound_direct_interval_r$root,lower_bound_direct_interval_r$root)
direct_interval_r
fnct_direct_r <- function(r, max_m_loglikelihood, gamma = 0.05) {
m <- m_estimate #mle for M under r is just m_estimate
f <-  (max_m_loglikelihood - calc_negbinom_loglik_profile(m = m ,r =r)) +1*log(gamma) #this value should be zero for the constraints to be satisfied
#replcae qchisq() with a random gamma cutoff if necesary
}
#here is an example of how to do direct likelihood for M
lower_bound_direct_interval_r <- uniroot(fnct_direct_r, c(0.1, r_estimate) , max_m_loglikelihood = max_m_loglikelihood)
lower_bound_direct_interval_r <- uniroot(fnct_direct_r, c(r_estimate, 0.9) , max_m_loglikelihood = max_m_loglikelihood)
direct_interval_r <- c(lower_bound_direct_interval_r$root,lower_bound_direct_interval_r$root)
direct_interval_r
fnct_direct_r <- function(r, max_m_loglikelihood, gamma = 0.15) {
m <- m_estimate #mle for M under r is just m_estimate
f <-  (max_m_loglikelihood - calc_negbinom_loglik_profile(m = m ,r =r)) +1*log(gamma) #this value should be zero for the constraints to be satisfied
#replcae qchisq() with a random gamma cutoff if necesary
}
#here is an example of how to do direct likelihood for M
lower_bound_direct_interval_r <- uniroot(fnct_direct_r, c(0.1, r_estimate) , max_m_loglikelihood = max_m_loglikelihood)
upper_bound_direct_interval_r <- uniroot(fnct_direct_r, c(r_estimate, 0.9) , max_m_loglikelihood = max_m_loglikelihood)
direct_interval_r <- c(lower_bound_direct_interval_r$root,upper_bound_direct_interval_r$root)
direct_interval_r
r_estimate
