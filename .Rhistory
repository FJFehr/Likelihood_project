mle_profile <- #find mle for r at each value of m
optim(
par = 1,
r = r_range[i],
fn = calc_negbinom_loglik_profile ,
control = list(fnscale = -1),
lower = 1,
upper = 100 ,
method = "L-BFGS-B"
)
estimated_m_s[i] <- mle_profile$par
}
return(estimated_m_s)
}
estimated_m_values <- profile_likelihood_calc_shape() #all m values are the same, this make sense as m is the dude who will afect loglikelihood
r_profile_loglik_values <- numeric(length(r_range))
for (i in seq_along(estimated_m_values)){
r_profile_loglik_values[i] <- calc_negbinom_loglik_profile(m = estimated_m_values[i], r = r_range[i])
}
data_frame(x = r_range, val = r_profile_loglik_values) %>%
ggplot(aes(x=x,y=val))+
geom_point(size = 1) +
scale_x_continuous(breaks = seq(0,0.8,by=0.05)) +
geom_vline(xintercept = 0.58, col = "red", lty = "dashed") +
labs(x = "R shape parameter value", y = "Profile Loglikelihood" , title = "Profile Loglikelihood of R Shape Parameter") +
theme_classic()
#I have calculated the score of M
score_U <- function(m){
r <- m^2 /(var(x)-m)
u <- (sum(x)*r-n*m*r)/(m*(r+m))
return (u)
}
thetas <- seq(0.001,20,by = 0.1)
score <- 0
for(i in 1:length(thetas)){
score[i] <- score_U(thetas[i])
}
df <- as.data.frame(thetas,score)
ggplot(df,aes(x = thetas,y = score))+
geom_line()+
geom_hline(yintercept = 0)+
geom_vline(xintercept = 6.9, col = "red", lty = "dashed")+
theme(plot.title = element_text(hjust=0.5))+
labs(title="Negative Binomial Score \n m parameter") +
labs(x="m", y="U(m)")
#according to my maths this would give us the fisher info (sent a picture via whatsapp)
fisher_info_m <- function(r,m){
info <- (n*r*m*(m+r)+sum(x)*r-n*m*r*(r+r*m))/(m^2*(r+m)^2)
return(info)
}
se_m <- sqrt(fisher_info_m(r_estimate,m_estimate)^(-1))
wald_m <- c(0,0)
wald_m[1] <- m_estimate-1.96*se_m
wald_m[2] <- m_estimate+1.96*se_m
# Here is an example to get a SE from the hessian but im unsure how since our LL has one parameter...
# fit<-optim(pars,li_func,control=list("fnscale"=-1),hessian=TRUE,...)
# fisher_info<-solve(-fit$hessian)
# prop_sigma<-sqrt(diag(fisher_info))
# prop_sigma<-diag(prop_sigma)
# upper<-fit$par+1.96*prop_sigma
# lower<-fit$par-1.96*prop_sigma
# interval<-data.frame(value=fit$par, upper=upper, lower=lower)
data_frame('Lower bound'= wald_m[1] , 'Upper bound' = wald_m[2]) %>% knitr::kable(caption = "Wald interval for m parameter")
#select all bounds which meet a cutoff,
#done for m only
#plug in mle values to get l(theta_hat)
max_m_loglikelihood <- calc_negbinom_loglik_profile(m = m_estimate , r = r_estimate)
fnct <- function(m, max_m_loglikelihood, cutoff = qchisq(.95,1)) {
# out <- #using function that i used to calculate joint MLE for m and R the only downside to this is that it is NOT taking in different shape parameters...check out the function
# optim(
# par = m,
# fn = calc_negbinom_loglik ,
# control = list(fnscale = -1),
# lower = 3,
# upper = 30 ,
# method = "L-BFGS-B"
# )
r <- m^2 /(var(x) - m )
f <- 2 * (max_m_loglikelihood - calc_negbinom_loglik_profile(m = m ,r = r)) - cutoff #this value should be zero for the constraints to be satisfied
#replcae qchisq() with a random gamma cutoff if necesary
}
lower_bound_wilk_interval <- uniroot(fnct, c(1, m_estimate) , max_m_loglikelihood = max_m_loglikelihood)
upper_bound_wilk_interval <- uniroot(fnct, c(m_estimate, 52) , max_m_loglikelihood = max_m_loglikelihood)
wilks_likelihood <- c(lower_bound_wilk_interval$root,upper_bound_wilk_interval$root)
data_frame('Lower bound'= wilks_likelihood[1] , 'Upper bound' = wilks_likelihood[2]) %>% knitr::kable(caption = "Wilks Likelihood interval for m parameter")
#likelihood ratio Graph
r_theta <- function(m){
diff <- exp(calc_negbinom_loglik(m)- calc_negbinom_loglik(m_estimate))
return(diff)
}
rthetas <- 0
thetas <- seq(5,10,by = 0.1)
for(i in 1:length(thetas)){
rthetas[i] <- r_theta(thetas[i])
}
df <- as.data.frame(thetas,rthetas)
ggplot(df,aes(x = thetas,y = rthetas))+
geom_line()+
geom_hline(yintercept = 0.15, col = "red", lty = "dashed")+
theme(plot.title = element_text(hjust=0.5))+
labs(title="Relative Likelihood for Negative Binomial") +
labs(x="m", y="Relative Likelihood")
fnct_direct <- function(m, max_m_loglikelihood, gamma = 0.15) {
# out <- #using function that i used to calculate joint MLE for m and R the only downside to this is that it is NOT taking in different shape parameters...check out the function
# optim(
# par = m,
# fn = calc_negbinom_loglik ,
# control = list(fnscale = -1),
# lower = 3,
# upper = 30 ,
# method = "L-BFGS-B"
# )
r <- m^2 /(var(x) - m )
f <-  (max_m_loglikelihood - calc_negbinom_loglik_profile(m = m ,r =r)) +1*log(gamma) #this value should be zero for the constraints to be satisfied
#replcae qchisq() with a random gamma cutoff if necesary
}
#here is an example of how to do direct likelihood for M
lower_bound_direct_interval_m <- uniroot(fnct_direct, c(1, m_estimate) , max_m_loglikelihood = max_m_loglikelihood)
upper_bound_direct_interval_m <- uniroot(fnct_direct, c(m_estimate, 52) , max_m_loglikelihood = max_m_loglikelihood)
direct_interval_m <- c(lower_bound_direct_interval_m$root,upper_bound_direct_interval_m$root)
data_frame('Lower bound'= direct_interval_m[1] , 'Upper bound' = direct_interval_m[2]) %>% knitr::kable(caption = " interval for m parameter")
fnct_direct_r <- function(r, max_m_loglikelihood, gamma = 0.15) {
m <- m_estimate #mle for M under r is just m_estimate
f <-  (max_m_loglikelihood - calc_negbinom_loglik_profile(m = m ,r =r)) +1*log(gamma) #this value should be zero for the constraints to be satisfied
#replcae qchisq() with a random gamma cutoff if necesary
}
#here is an example of how to do direct likelihood for M
lower_bound_direct_interval_r <- uniroot(fnct_direct_r, c(0.1, r_estimate) , max_m_loglikelihood = max_m_loglikelihood)
upper_bound_direct_interval_r <- uniroot(fnct_direct_r, c(r_estimate, 0.9) , max_m_loglikelihood = max_m_loglikelihood)
direct_interval_r <- c(lower_bound_direct_interval_r$root,upper_bound_direct_interval_r$root)
intervals <- c("Wald Interval", "Wilks Likelihood","Direct Likelihood 15%")
lowerIntervals <- c(wald_m[1], wilks_likelihood[1], direct_interval_m[1])
upperIntervals <- c(wald_m[2], wilks_likelihood[2], direct_interval_m[2])
data_frame(Intervals = intervals, "Lower Bound" = lowerIntervals, "Upper Bound"= upperIntervals) %>%
knitr::kable(format = "latex",
caption = "Intervals for m parameter",
booktabs = T)
se_m
mle_negbinom$hessian
sqrt((-mle_negbinom$hessian)^(-1))
Z <- matrix(z_range_ll,nrow=100)
cont_plot <- contour(m_range_ll, r_range_ll, Z, add = F, nlevels = 30,
xlab="m values", ylab="r values", zlab="Likelihood")
persp_plot <- persp(m_range_ll,r_range_ll, Z, theta=-50, phi=15,
col="lightblue",expand = 0.5,shade = 0.2,
xlab="m values", ylab="r values", zlab="Likelihood")
cont_plot
persp_plot
cont_plot <- contour(m_range_ll, r_range_ll, Z, add = F, nlevels = 30,
xlab="m values", ylab="r values", zlab="Likelihood")
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, fig.width = 6, fig.height = 5, fig.pos="H", fig.pos = 'H')
# Note: Include = FALSE implies the code is executed, but not printed in your pdf.
# warning and message = FALSE implies ugly messages and warnings are removed from your pdf. These should be picked up when you execute the command chunks (code sections below) in your rmd, not printed in your paper!
library(knitr)
library(ggplot2)
library(kableExtra)
library(tidyverse)
library(cowplot) #for plotting cows
library(ZIM) #for generating observations
library(fitdistrplus)
library(jpeg)# for pulling in our likelihood surface
#
# devtools::install_version("rmarkdown", version = "1.8", repos = "http://cran.us.r-project.org")
dat <- read.csv("data/accidents.csv") %>%
setNames(c("Index","Counts")) %>%
as_data_frame() %>%
mutate(Counts = as.numeric(Counts))
#define usefule variables
x <- dat$Counts
n <- nrow(dat)
#first column is just an index, second is a  1632 observations
#first visualise the data
distribution_plot <- ggplot(dat, aes(Counts)) +
stat_count( fill="blue",
alpha = .4,
position="dodge")+
labs(x="Accident counts", y="Frequency" , title="Histogram\n Accident counts") +
theme_classic() +
theme(plot.title = element_text(hjust=0.5))
firstbox <- ggplot(dat)+
geom_boxplot(aes(y= dat$Counts,x=1),
fill = "cornflowerblue",
outlier.color ="firebrick")+
theme_classic() +
theme(plot.title = element_text(hjust=0.5))+
labs(title="Boxplot\n Accident counts") +
labs(y="Accident counts",x ="")+
coord_flip()
plot_grid(distribution_plot,firstbox,ncol = 1)
percentage_zero <- ((sum(dat$Counts==0)/nrow(dat))*100) %>% round(digits = 2)
summarize(.data = dat, Mean= round(mean(Counts),4), Variance = round(var(Counts),4), Median = median(Counts)) %>% knitr::kable(caption = "Summary statisitics")
#percentage outliers - 3rd quartile is 9
perc_outlier <- round(length(dat$Counts[dat$Counts > 1.5*9])/length(dat$Counts),4)*100
#five number summary of the data
five_sum <- summary(dat)
#we can use mle to fit this
mle_lambda <- round(sum(x)/n,4)
loglik_poisson <- -n*mle_lambda + sum(x)*log(mle_lambda)  -sum(log(factorial(x)))
aic_poisson <- round(-2*loglik_poisson + 2*(1),2)
bic_poisson <- round(-2*loglik_poisson + log(n),2)
data_frame(`$\\hat{\\lambda}$`= mle_lambda , AIC = aic_poisson, BIC = bic_poisson) %>% knitr::kable(caption = "Poisson MLE's & information metrics")
#https://en.wikipedia.org/wiki/Negative_binomial_distribution#Gamma%E2%80%93Poisson_mixture
#use optim to optimise for r and m
#r is a shape parameter so it doesnt really affect the loglikelihood much
# we want mean = variance i think... therefore you can solve for r = m^2 /(sample variance  - m)
#This is done by looking at birgit's notes and seeing that variance = m + m^2/r
calc_negbinom_loglik <- function(par){
m <- par[1]
r <- m^2 /(var(x)-m)
loglik <- -n*log(gamma(r)) + sum(log(gamma(r+x))) - sum(log(factorial(x))) + sum(x) * log(m/(m+r)) + n*r*log(r/(r+m))
return(loglik)
}
#If we maximise loglikelihood, we will be minimising AIC
mle_negbinom <-
optim(
par = 1,
fn = calc_negbinom_loglik ,
control = list(fnscale = -1),
lower = 3,
upper = 30 ,
method = "L-BFGS-B",
hessian = T
)
m_estimate <- round(mle_negbinom$par[1],4)
r_estimate <- round(m_estimate^2 /(var(x)-m_estimate),4)
aic_negbinom <- round(-2*mle_negbinom$value + 2*(2),2)
bic_negbinom <- round(-2*mle_negbinom$value + log(n),2)
data_frame(`Mean $\\hat{m}$`= m_estimate , `Shape $\\hat{r}$` = r_estimate, AIC = aic_negbinom, BIC = bic_negbinom) %>% knitr::kable(caption = "Negative Binomial MLE's & information metrics")
calc_poissmix_loglik <- function(par){
lambda_1 <- par[1]
lambda_2 <- par[2]
p <- par[3]
loglik <- sum(log(p * dpois(x = x, lambda = lambda_1) + (1-p)*dpois(x = x, lambda = lambda_2)))
return(loglik)
}
#If we maximise loglikelihood, we will be minimising AIC
mle_poismix <-
optim(
par = c(25,10,0.5),
fn = calc_poissmix_loglik ,
control = list(fnscale = -1), #maximise
lower = c(0.1,0.1,0),
upper = c(100,100,1) ,
method = "L-BFGS-B"
)
lambda_1_estimate <- round(mle_poismix$par[1],4)
lambda_2_estimate <- round(mle_poismix$par[2],4)
p_estimate <- round(mle_poismix$par[3],4)
aic_poismix <- round(-2*mle_poismix$value + 2*(3),2)
bic_poismix <- round(-2*mle_poismix$value + log(n),2)
data_frame(`$\\hat{\\lambda_1}$`= lambda_1_estimate , `$\\hat{\\lambda_2}$` = lambda_2_estimate, `Proportion p` = p_estimate, AIC = aic_poismix, BIC = bic_poismix) %>% knitr::kable(caption = "Poisson-Poisson MLE's & information metrics")
calc_zip_loglik <- function(par){
lambda <- par[1]
pi <- par[2]
#this for loop is slightly inefficient but it allows us to see what is happening
loglik <- 0
for(j in x){
indicator <- as.numeric(j > 0) #assign value 1 if greater than 0
loglik <- loglik + log( (1 - indicator)*(pi + (1-pi)*exp(-lambda)) + indicator*((1-pi) * dpois(x = j , lambda = lambda))   )
}
return(loglik)
}
#If we maximise loglikelihood, we will be minimising AIC
mle_zip <-
optim(
par = c(10,0.2),
fn = calc_zip_loglik ,
control = list(fnscale = -1), #maximise
lower = c(0.1,0),
upper = c(100,0.99999) ,
method = "L-BFGS-B"
)
lambda_zip_estimate <- round(mle_zip$par[1],4)
pi_zip_estimate <- round(mle_zip$par[2],4)
aic_zip <- round(-2*mle_zip$value + 2*(2),2)
bic_zip <- round(-2*mle_zip$value + log(n),2)
data_frame(`$\\hat{\\lambda}$`= lambda_zip_estimate , `$\\hat{\\pi}$` = pi_zip_estimate,  AIC = aic_zip, BIC = bic_zip) %>%
knitr::kable(caption = "Zero inflated Poisson MLE's & information metrics")
aics <- c(aic_poisson,aic_negbinom,aic_poismix,aic_zip)
bics <- c(bic_poisson,bic_negbinom,bic_poismix,bic_zip)
model <- c("Poisson","Negative Binomial","Poisson Mixture","Zero Inflated Poisson")
data_frame(Model = model, AIC = aics,BIC = bics) %>%
knitr::kable(format = "latex",
caption = "Models fitted to accident data",
booktabs = T) %>%
row_spec(2, bold = T)
#Here are some 3d plots from Birgits code - since r is a function of m it has no affect in the plots. But they still cool just take a soek to run
# negbinom_loglik_plotting <- function(par){
#   m <- par[1]
#   r <- par[2]
#
#   loglik <- -n*log(gamma(r)) + sum(log(gamma(r+x))) - sum(log(factorial(x))) + sum(x) * log(m/(m+r)) + n*r*log(r/(r+m))
#
#   return(loglik)
#
# }
#
# #m range for plotting
# m_range_ll <- seq(1,20, length.out = 100)
# #r range for plotting
# r_range_ll <- seq(0.01,1,length.out = 100)
#
# #every combination - kinda like a kronecker product
# pp <- expand.grid(x = m_range_ll, y = r_range_ll)
#
# #loglike z range for plotting
# z_range_ll <- numeric(length(pp$x))
#
# for(i in 1:length(pp$x)){
#    z_range_ll[i] <- negbinom_loglik_plotting(pp[i,])
#  }
#
# z_range_ll <- unlist(z_range_ll)
#
# Z <- matrix(z_range_ll,nrow=100)
#
# #Saves plot
# jpeg('contourplot.jpg')
#   contour(m_range_ll, r_range_ll, Z, add = F, nlevels = 30,
#        xlab="m values", ylab="r values", zlab="Likelihood")
# dev.off()
#
# jpeg('perspplot.jpg')
#   persp(m_range_ll,r_range_ll, Z, theta=-50, phi=15,
#          col="lightblue",expand = 0.5,shade = 0.2,
#          xlab="m values", ylab="r values", zlab="Likelihood")
# dev.off()
#read file
img <- readJPEG("contourplot.jpg",native = TRUE)
if(exists("rasterImage")){
cont <- plot(1:2, type='n',axes = F,xlab = "",ylab = "",main = "Negative Binomial likelihood surface")
rasterImage(img,1,1,2,2)
}
img <- readJPEG("perspplot.jpg",native = TRUE)
if(exists("rasterImage")){
persp <- plot(1:2, type='n',axes = F,xlab = "",ylab = "",main = "Negative Binomial likelihood surface")
rasterImage(img,1,1,2,2)
}
#overlay distributions
#generate zip_predictions
# generate_zip_prob <- function(lambda,pi,x){
#
#   if(x == 0){
#     return(pi + (1-pi)*exp(-lambda))
#   }else{
#     return((1-pi)*dpois(x=x,lambda=lambda))
#
#   }
#
# }
generate_negbinom_prob <- function(m,r,x){
(gamma(r + x))/(factorial(x)*gamma(r)) * (m/(m+r))^(x) * (r /(r+m))^(r)
}
probs <- numeric(81)
for (i in 0:80){
probs[i+1] <- generate_negbinom_prob(m = m_estimate , r = r_estimate , x=i)
}
#get expected frequencies
expected_freq <- probs*n
comparison_df <- table(dat$Counts) %>%
as_data_frame %>%
rename(accident_count = Var1,
observed = n) %>%
mutate(expected = expected_freq[1:56],
accident_count = as.numeric(accident_count)) %>%  #take only first 56 values
gather(Type,`Frequency` ,c(observed,expected))
ggplot(comparison_df, aes(x = accident_count , y = Frequency , fill = Type )) +
geom_bar( stat="identity",alpha = .8,
position="dodge") +
labs(x="Accident counts", y="Frequency" , title="Observed vs Expected Frequencies as per Negative Binomial") +
theme_classic() +
theme(plot.title = element_text(hjust=0.5))
#check if any of this is right
fit_nbinom <- fitdist(data = x , method = "mle","nbinom")
chitest <- chisq.test(filter(comparison_df,Type == "observed")$Frequency,filter(comparison_df,Type != "observed")$Frequency)
data_frame(`$\\chi^2$ Statistic`= chitest$statistic , `$\\chi^2$ DoF` = chitest$parameter,  "P-Value"= round(chitest$p.value,2)) %>%
knitr::kable(caption = "Pearson's $\\chi^2$ Goodness of Fit test")
ggplot(data = dat, aes(sample =dat$Counts))+
geom_abline()+
stat_qq(distribution = stats::qnbinom,
dparams = list(r_estimate,mu = m_estimate),
col = "turquoise4", alpha=0.4)+
theme_classic() +
theme(plot.title = element_text(hjust=0.5))+
labs(title="Q-Q plot for Negative Binomial")
#need to calulcate mle at EACH point
m_range <- seq(6,8, length.out = 100) #values of m for profile m loglikelihood
r_range <- seq(0.5,0.7 , length.out = 100)
calc_negbinom_loglik_profile <- function(r,m){
loglik <- -n*log(gamma(r)) + sum(log(gamma(r+x))) - sum(log(factorial(x))) + sum(x) * log(m/(m+r)) + n*r*log(r/(r+m))
return(loglik)
}
profile_likelihood_calc_mean <- function(){
#Calculate profile for M by calculating mle for R at eacg value of M
estimated_r_s <- numeric(length(m_range))
for(i in seq_along(m_range)){
mle_profile <- #find mle for r at each value of m
optim(
par = 1,
m = m_range[i],
fn = calc_negbinom_loglik_profile ,
control = list(fnscale = -1),
lower = 0.1,
upper = 30 ,  # i presume shape cant take a ridiculous value like this
method = "L-BFGS-B"
)
estimated_r_s[i] <- mle_profile$par
}
return(estimated_r_s)
}
estimated_r_values <- profile_likelihood_calc_mean()
#For each r in estimated_r_values, sub in M from 1:56 and trace it
m_profile_loglik_values <- numeric(length(m_range))
for (i in seq_along(estimated_r_values)){
m_profile_loglik_values[i] <- calc_negbinom_loglik_profile(r = estimated_r_values[i], m = m_range[i])
}
data_frame(x = m_range, val = m_profile_loglik_values) %>%
ggplot(aes(x=x,y=val))+
geom_point(size = 1) +
scale_x_continuous(breaks = seq(0,56,by=2)) +
geom_vline(xintercept = 6.9, col = "red", lty = "dashed") +
labs(x = "M parameter value", y = "Profile Loglikelihood" , title = "Profile Loglikelihood of M Parameter") +
theme_classic()
#Now we do the profile loglikelihood for the r
#I have calculated the score of M
score_U <- function(m){
r <- m^2 /(var(x)-m)
u <- (sum(x)*r-n*m*r)/(m*(r+m))
return (u)
}
thetas <- seq(0.001,20,by = 0.1)
score <- 0
for(i in 1:length(thetas)){
score[i] <- score_U(thetas[i])
}
df <- as.data.frame(thetas,score)
ggplot(df,aes(x = thetas,y = score))+
geom_line()+
geom_hline(yintercept = 0)+
geom_vline(xintercept = 6.9, col = "red", lty = "dashed")+
theme(plot.title = element_text(hjust=0.5))+
labs(title="Negative Binomial Score \n m parameter") +
labs(x="m", y="U(m)")
#have to redfine function to allow for correct optimisation
calc_negbinom_loglik_profile <- function(m,r){
loglik <- -n*log(gamma(r)) + sum(log(gamma(r+x))) - sum(log(factorial(x))) + sum(x) * log(m/(m+r)) + n*r*log(r/(r+m))
return(loglik)
}
profile_likelihood_calc_shape <- function(){
#Calculate profile for R by calculating mle for M at eacg value of R
estimated_m_s <- numeric(length(r_range))
for(i in seq_along(r_range)){
mle_profile <- #find mle for r at each value of m
optim(
par = 1,
r = r_range[i],
fn = calc_negbinom_loglik_profile ,
control = list(fnscale = -1),
lower = 1,
upper = 100 ,
method = "L-BFGS-B"
)
estimated_m_s[i] <- mle_profile$par
}
return(estimated_m_s)
}
estimated_m_values <- profile_likelihood_calc_shape() #all m values are the same, this make sense as m is the dude who will afect loglikelihood
r_profile_loglik_values <- numeric(length(r_range))
r_profile_ratio <- numeric(length(r_range))
for (i in seq_along(estimated_m_values)){
r_profile_loglik_values[i] <- calc_negbinom_loglik_profile(m = estimated_m_values[i], r = r_range[i])
#Going to try make relative likelihood ratio
r_profile_ratio[i] <- exp(r_profile_loglik_values[i] - calc_negbinom_loglik_profile(m = estimated_m_values[i], r = r_estimate))
}
data_frame(x = r_range, val = r_profile_loglik_values) %>%
ggplot(aes(x=x,y=val))+
geom_point(size = 1) +
scale_x_continuous(breaks = seq(0,0.8,by=0.05)) +
geom_vline(xintercept = 0.58, col = "red", lty = "dashed") +
labs(x = "R shape parameter value", y = "Profile Loglikelihood" , title = "Profile Loglikelihood of R Shape Parameter") +
theme_classic()
calc_negbinom_loglik_profile()
calc_negbinom_loglik_profile
r_profile_mle <-
optim(
par = 1,
m = m_estimate,
fn = calc_negbinom_loglik_profile ,
control = list(fnscale = -1),
lower = 0.5,
upper = 0.7 ,
method = "L-BFGS-B",
hessian = T
)
r_profile_mle
se_r <- sqrt((-r_profile_mle$value)^(-1))
se_r
wald_r <- c(0,0)
wald_r[1] <- r_estimate - 1.96 * se_r
wald_r[2] <- r_estimate + 1.96 * se_r
wald_r
fnct_wilks_r <- function(r, max_m_loglikelihood, cutoff = qchisq(.95,1)) {
f <- 2 * (max_m_loglikelihood - calc_negbinom_loglik_profile(m = m_estimate ,r = r)) - cutoff #this value should be zero for the constraints to be satisfied
#replcae qchisq() with a random gamma cutoff if necesary
}
fnct_wilks_r <- function(r, max_m_loglikelihood, cutoff = qchisq(.95,1)) {
f <- 2 * (max_m_loglikelihood - calc_negbinom_loglik_profile(m = m_estimate ,r = r)) - cutoff #this value should be zero for the constraints to be satisfied
#replcae qchisq() with a random gamma cutoff if necesary
}
lower_bound_wilk_interval_r <- uniroot(fnct_wilks_r, c(0.2, r_estimate) , max_m_loglikelihood = max_m_loglikelihood)
upper_bound_wilk_interval_r <- uniroot(fnct_wilks_r, c(r_estimate, 0.7) , max_m_loglikelihood = max_m_loglikelihood)
lower_bound_wilk_interval_r
