as_data_frame() %>%
mutate(Counts = as.numeric(Counts))
#define usefule variables
x <- dat$Counts
n <- nrow(dat)
#first column is just an index, second is a  1632 observations
#first visualise the data
distribution_plot <- ggplot(dat, aes(Counts)) +
stat_count( fill="blue",
alpha = .4,
position="dodge")+
labs(x="Accident counts", y="Frequency" , title="Histogram\n Accident counts") +
theme_classic() +
theme(plot.title = element_text(hjust=0.5))
firstbox <- ggplot(dat)+
geom_boxplot(aes(y= dat$Counts,x=1),
fill = "cornflowerblue",
outlier.color ="firebrick")+
theme_classic() +
theme(plot.title = element_text(hjust=0.5))+
labs(title="Boxplot\n Accident counts") +
labs(y="Accident counts",x ="")+
coord_flip()
plot_grid(distribution_plot,firstbox,ncol = 1)
percentage_zero <- ((sum(dat$Counts==0)/nrow(dat))*100) %>% round(digits = 2)
summarize(.data = dat, Mean= mean(Counts), Variance = var(Counts), Median = median(Counts)) %>% knitr::kable()
#five number summary of the data
five_sum <- summary(dat)
#percentage outliers - 3rd quartile is 9
perc_outlier <- round(length(dat$Counts[dat$Counts > 1.5*9])/length(dat$Counts),4)*100
#we can use mle to fit this
mle_lambda <- sum(x)/n
loglik_poisson <- -n*mle_lambda + sum(x)*log(mle_lambda)  -sum(log(factorial(x)))
aic_poisson <- -2*loglik_poisson + 2*(1)
data_frame(`Lambda Estimate`= mle_lambda , AIC = aic_poisson) %>% knitr::kable()
#https://en.wikipedia.org/wiki/Negative_binomial_distribution#Gamma%E2%80%93Poisson_mixture
#use optim to optimise for r and m
#r is a shape parameter so it doesnt really affect the loglikelihood much
# we want mean = variance i think... therefore you can solve for r = m^2 /(sample variance  - m)
#This is done by looking at birgit's notes and seeing that variance = m + m^2/r
calc_negbinom_loglik <- function(par){
m <- par[1]
r <- m^2 /(var(x)-m)
loglik <- -n*log(gamma(r)) + sum(log(gamma(r+x))) - sum(log(factorial(x))) + sum(x) * log(m/(m+r)) + n*r*log(r/(r+m))
return(loglik)
}
#If we maximise loglikelihood, we will be minimising AIC
mle_negbinom <-
optim(
par = 1,
fn = calc_negbinom_loglik ,
control = list(fnscale = -1),
lower = 3,
upper = 30 ,
method = "L-BFGS-B"
)
m_estimate <- mle_negbinom$par[1]
r_estimate <- m_estimate^2 /(var(x)-m_estimate)
aic_negbinom <- -2*mle_negbinom$value + 2*(2)
data_frame(`M Mean Estimate`= m_estimate , `r Shape Estimate` = r_estimate, AIC = aic_negbinom) %>% knitr::kable()
calc_poissmix_loglik <- function(par){
lambda_1 <- par[1]
lambda_2 <- par[2]
p <- par[3]
loglik <- sum(log(p * dpois(x = x, lambda = lambda_1) + (1-p)*dpois(x = x, lambda = lambda_2)))
return(loglik)
}
#If we maximise loglikelihood, we will be minimising AIC
mle_poismix <-
optim(
par = c(25,10,0.5),
fn = calc_poissmix_loglik ,
control = list(fnscale = -1), #maximise
lower = c(0.1,0.1,0),
upper = c(100,100,1) ,
method = "L-BFGS-B"
)
lambda_1_estimate <- mle_poismix$par[1]
lambda_2_estimate <- mle_poismix$par[2]
p_estimate <- mle_poismix$par[3]
aic_poismix <- -2*mle_poismix$value + 2*(3)
data_frame(`Lamda 1 estimate`= lambda_1_estimate , `Lambda 2 estimate` = lambda_2_estimate, `Mixing Parameter p` = p_estimate, AIC = aic_poismix) %>% knitr::kable()
calc_zip_loglik <- function(par){
lambda <- par[1]
pi <- par[2]
#this for loop is slightly inefficient but it allows us to see what is happening
loglik <- 0
for(j in x){
indicator <- as.numeric(j > 0) #assign value 1 if greater than 0
loglik <- loglik + log( (1 - indicator)*(pi + (1-pi)*exp(-lambda)) + indicator*((1-pi) * dpois(x = j , lambda = lambda))   )
}
return(loglik)
}
#If we maximise loglikelihood, we will be minimising AIC
mle_zip <-
optim(
par = c(10,0.2),
fn = calc_zip_loglik ,
control = list(fnscale = -1), #maximise
lower = c(0.1,0),
upper = c(100,0.99999) ,
method = "L-BFGS-B"
)
lambda_zip_estimate <- mle_zip$par[1]
pi_zip_estimate <- mle_zip$par[2]
aic_zip <- -2*mle_zip$value + 2*(2)
data_frame(`Lamda estimate`= lambda_zip_estimate , `Pi estimate` = pi_zip_estimate,  AIC = aic_zip) %>% knitr::kable()
aics <- c(aic_poisson,aic_negbinom,aic_poismix,aic_zip)
model <- c("Poisson","Negative Binomial","Poisson Mixture","Zero Inflated Poisson")
data_frame(Model = model, AIC = aics) %>% knitr::kable()
#overlay distributions
#generate zip_predictions
# generate_zip_prob <- function(lambda,pi,x){
#
#   if(x == 0){
#     return(pi + (1-pi)*exp(-lambda))
#   }else{
#     return((1-pi)*dpois(x=x,lambda=lambda))
#
#   }
#
# }
generate_negbinom_prob <- function(m,r,x){
(gamma(r + x))/(factorial(x)*gamma(r)) * (m/(m+r))^(x) * (r /(r+m))^(r)
}
probs <- numeric(81)
for (i in 0:80){
probs[i+1] <- generate_negbinom_prob(m = m_estimate , r = r_estimate , x=i)
}
#get expected frequencies
expected_freq <- probs*n
comparison_df <- table(dat$Counts) %>%
as_data_frame %>%
rename(accident_count = Var1,
observed = n) %>%
mutate(expected = expected_freq[1:56],
accident_count = as.numeric(accident_count)) %>%  #take only first 56 values
gather(Type,`Frequency` ,c(observed,expected))
ggplot(comparison_df, aes(x = accident_count , y = Frequency , fill = Type )) +
geom_bar( stat="identity",alpha = .8,
position="dodge") +
labs(x="Accident counts", y="Frequency" , title="Observed vs Expected Frequencies as per Negative Binomial") +
theme_classic() +
theme(plot.title = element_text(hjust=0.5))
#qq plot
#check if any of this is right
fit_nbinom <- fitdist(data = x , method = "mle","nbinom")
# q-q
fitdistrplus::qqcomp(fit_nbinom,addlegend = F, main = "Q-Q plot for Negative Binomial")
chisq.test(filter(comparison_df,Type == "observed")$Frequency,filter(comparison_df,Type != "observed")$Frequency)
#need to calulcate mle at EACH point
m_range <- seq(1,56, by = 0.1) #values of m for profile m loglikelihood
r_range <- seq(0.01,0.8 , by = 0.01)
calc_negbinom_loglik_profile <- function(r,m){
loglik <- -n*log(gamma(r)) + sum(log(gamma(r+x))) - sum(log(factorial(x))) + sum(x) * log(m/(m+r)) + n*r*log(r/(r+m))
return(loglik)
}
profile_likelihood_calc_mean <- function(){
#Calculate profile for M by calculating mle for R at eacg value of M
estimated_r_s <- numeric(length(m_range))
for(i in seq_along(m_range)){
mle_profile <- #find mle for r at each value of m
optim(
par = 1,
m = m_range[i],
fn = calc_negbinom_loglik_profile ,
control = list(fnscale = -1),
lower = 0.1,
upper = 30 ,  # i presume shape cant take a ridiculous value like this
method = "L-BFGS-B"
)
estimated_r_s[i] <- mle_profile$par
}
return(estimated_r_s)
}
estimated_r_values <- profile_likelihood_calc_mean()
#For each r in estimated_r_values, sub in M from 1:56 and trace it
m_profile_loglik_values <- numeric(length(m_range))
for (i in seq_along(estimated_r_values)){
m_profile_loglik_values[i] <- calc_negbinom_loglik_profile(r = estimated_r_values[i], m = m_range[i])
}
data_frame(x = m_range, val = m_profile_loglik_values) %>%
ggplot(aes(x=x,y=val))+
geom_point(size = 1) +
scale_x_continuous(breaks = seq(0,56,by=2)) +
geom_vline(xintercept = 6.9, col = "red", lty = "dashed") +
labs(x = "M parameter value", y = "Profile Loglikelihood" , title = "Profile Loglikelihood of M Parameter") +
theme_classic()
#Now we do the profile loglikelihood for the r
#have to redfine function to allow for correct optimisation
calc_negbinom_loglik_profile <- function(m,r){
loglik <- -n*log(gamma(r)) + sum(log(gamma(r+x))) - sum(log(factorial(x))) + sum(x) * log(m/(m+r)) + n*r*log(r/(r+m))
return(loglik)
}
profile_likelihood_calc_shape <- function(){
#Calculate profile for R by calculating mle for M at eacg value of R
estimated_m_s <- numeric(length(r_range))
for(i in seq_along(r_range)){
mle_profile <- #find mle for r at each value of m
optim(
par = 1,
r = r_range[i],
fn = calc_negbinom_loglik_profile ,
control = list(fnscale = -1),
lower = 1,
upper = 100 ,
method = "L-BFGS-B"
)
estimated_m_s[i] <- mle_profile$par
}
return(estimated_m_s)
}
estimated_m_values <- profile_likelihood_calc_shape() #all m values are the same, this make sense as m is the dude who will afect loglikelihood
r_profile_loglik_values <- numeric(length(r_range))
for (i in seq_along(estimated_m_values)){
r_profile_loglik_values[i] <- calc_negbinom_loglik_profile(m = estimated_m_values[i], r = r_range[i])
}
data_frame(x = r_range, val = r_profile_loglik_values) %>%
ggplot(aes(x=x,y=val))+
geom_point(size = 1) +
scale_x_continuous(breaks = seq(0,0.8,by=0.05)) +
geom_vline(xintercept = 0.58, col = "red", lty = "dashed") +
labs(x = "R shape parameter value", y = "Profile Loglikelihood" , title = "Profile Loglikelihood of R Shape Parameter") +
theme_classic()
#https://en.wikipedia.org/wiki/Negative_binomial_distribution#Gamma%E2%80%93Poisson_mixture
#use optim to optimise for r and m
#r is a shape parameter so it doesnt really affect the loglikelihood much
# we want mean = variance i think... therefore you can solve for r = m^2 /(sample variance  - m)
#This is done by looking at birgit's notes and seeing that variance = m + m^2/r
calc_negbinom_loglik <- function(par){
m <- par[1]
r <- m^2 /(var(x)-m)
loglik <- -n*log(gamma(r)) + sum(log(gamma(r+x))) - sum(log(factorial(x))) + sum(x) * log(m/(m+r)) + n*r*log(r/(r+m))
return(loglik)
}
#If we maximise loglikelihood, we will be minimising AIC
mle_negbinom <-
optim(
par = 1,
fn = calc_negbinom_loglik ,
control = list(fnscale = -1),
lower = 3,
upper = 30 ,
method = "L-BFGS-B"
)
m_estimate <- mle_negbinom$par[1]
r_estimate <- m_estimate^2 /(var(x)-m_estimate)
aic_negbinom <- -2*mle_negbinom$value + 2*(2)
data_frame(`M Mean Estimate`= m_estimate , `r Shape Estimate` = r_estimate, AIC = aic_negbinom) %>% knitr::kable()
#https://en.wikipedia.org/wiki/Negative_binomial_distribution#Gamma%E2%80%93Poisson_mixture
#use optim to optimise for r and m
#r is a shape parameter so it doesnt really affect the loglikelihood much
# we want mean = variance i think... therefore you can solve for r = m^2 /(sample variance  - m)
#This is done by looking at birgit's notes and seeing that variance = m + m^2/r
calc_negbinom_loglik <- function(par){
m <- par[1]
r <- m^2 /(var(x)-m)
loglik <- -n*log(gamma(r)) + sum(log(gamma(r+x))) - sum(log(factorial(x))) + sum(x) * log(m/(m+r)) + n*r*log(r/(r+m))
return(loglik)
}
#If we maximise loglikelihood, we will be minimising AIC
mle_negbinom <-
optim(
par = 1,
fn = calc_negbinom_loglik ,
control = list(fnscale = -1),
lower = 3,
upper = 30 ,
method = "L-BFGS-B"
)
m_estimate <- mle_negbinom$par[1]
r_estimate <- m_estimate^2 /(var(x)-m_estimate)
aic_negbinom <- -2*mle_negbinom$value + 2*(2)
data_frame(`M Mean Estimate`= m_estimate , `r Shape Estimate` = r_estimate, AIC = aic_negbinom) %>% knitr::kable()
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, fig.width = 6, fig.height = 5, fig.pos="H", fig.pos = 'H')
# Note: Include = FALSE implies the code is executed, but not printed in your pdf.
# warning and message = FALSE implies ugly messages and warnings are removed from your pdf. These should be picked up when you execute the command chunks (code sections below) in your rmd, not printed in your paper!
library(knitr)
library(ggplot2)
library(kableExtra)
library(tidyverse)
library(cowplot)
library(ZIM) #for generating observations
library(fitdistrplus)
#
# devtools::install_version("rmarkdown", version = "1.8", repos = "http://cran.us.r-project.org")
dat <- read.csv("data/accidents.csv") %>%
setNames(c("Index","Counts")) %>%
as_data_frame() %>%
mutate(Counts = as.numeric(Counts))
#define usefule variables
x <- dat$Counts
n <- nrow(dat)
#first column is just an index, second is a  1632 observations
#first visualise the data
distribution_plot <- ggplot(dat, aes(Counts)) +
stat_count( fill="blue",
alpha = .4,
position="dodge")+
labs(x="Accident counts", y="Frequency" , title="Histogram\n Accident counts") +
theme_classic() +
theme(plot.title = element_text(hjust=0.5))
firstbox <- ggplot(dat)+
geom_boxplot(aes(y= dat$Counts,x=1),
fill = "cornflowerblue",
outlier.color ="firebrick")+
theme_classic() +
theme(plot.title = element_text(hjust=0.5))+
labs(title="Boxplot\n Accident counts") +
labs(y="Accident counts",x ="")+
coord_flip()
plot_grid(distribution_plot,firstbox,ncol = 1)
percentage_zero <- ((sum(dat$Counts==0)/nrow(dat))*100) %>% round(digits = 2)
summarize(.data = dat, Mean= mean(Counts), Variance = var(Counts), Median = median(Counts)) %>% knitr::kable()
#five number summary of the data
five_sum <- summary(dat)
#percentage outliers - 3rd quartile is 9
perc_outlier <- round(length(dat$Counts[dat$Counts > 1.5*9])/length(dat$Counts),4)*100
#we can use mle to fit this
mle_lambda <- sum(x)/n
loglik_poisson <- -n*mle_lambda + sum(x)*log(mle_lambda)  -sum(log(factorial(x)))
aic_poisson <- -2*loglik_poisson + 2*(1)
data_frame(`Lambda Estimate`= mle_lambda , AIC = aic_poisson) %>% knitr::kable()
#https://en.wikipedia.org/wiki/Negative_binomial_distribution#Gamma%E2%80%93Poisson_mixture
#use optim to optimise for r and m
#r is a shape parameter so it doesnt really affect the loglikelihood much
# we want mean = variance i think... therefore you can solve for r = m^2 /(sample variance  - m)
#This is done by looking at birgit's notes and seeing that variance = m + m^2/r
calc_negbinom_loglik <- function(par){
m <- par[1]
r <- m^2 /(var(x)-m)
loglik <- -n*log(gamma(r)) + sum(log(gamma(r+x))) - sum(log(factorial(x))) + sum(x) * log(m/(m+r)) + n*r*log(r/(r+m))
return(loglik)
}
#If we maximise loglikelihood, we will be minimising AIC
mle_negbinom <-
optim(
par = 1,
fn = calc_negbinom_loglik ,
control = list(fnscale = -1),
lower = 3,
upper = 30 ,
method = "L-BFGS-B"
)
m_estimate <- mle_negbinom$par[1]
r_estimate <- m_estimate^2 /(var(x)-m_estimate)
aic_negbinom <- -2*mle_negbinom$value + 2*(2)
data_frame(`M Mean Estimate`= m_estimate , `r Shape Estimate` = r_estimate, AIC = aic_negbinom) %>% knitr::kable()
calc_poissmix_loglik <- function(par){
lambda_1 <- par[1]
lambda_2 <- par[2]
p <- par[3]
loglik <- sum(log(p * dpois(x = x, lambda = lambda_1) + (1-p)*dpois(x = x, lambda = lambda_2)))
return(loglik)
}
#If we maximise loglikelihood, we will be minimising AIC
mle_poismix <-
optim(
par = c(25,10,0.5),
fn = calc_poissmix_loglik ,
control = list(fnscale = -1), #maximise
lower = c(0.1,0.1,0),
upper = c(100,100,1) ,
method = "L-BFGS-B"
)
lambda_1_estimate <- mle_poismix$par[1]
lambda_2_estimate <- mle_poismix$par[2]
p_estimate <- mle_poismix$par[3]
aic_poismix <- -2*mle_poismix$value + 2*(3)
data_frame(`Lamda 1 estimate`= lambda_1_estimate , `Lambda 2 estimate` = lambda_2_estimate, `Mixing Parameter p` = p_estimate, AIC = aic_poismix) %>% knitr::kable()
calc_zip_loglik <- function(par){
lambda <- par[1]
pi <- par[2]
#this for loop is slightly inefficient but it allows us to see what is happening
loglik <- 0
for(j in x){
indicator <- as.numeric(j > 0) #assign value 1 if greater than 0
loglik <- loglik + log( (1 - indicator)*(pi + (1-pi)*exp(-lambda)) + indicator*((1-pi) * dpois(x = j , lambda = lambda))   )
}
return(loglik)
}
#If we maximise loglikelihood, we will be minimising AIC
mle_zip <-
optim(
par = c(10,0.2),
fn = calc_zip_loglik ,
control = list(fnscale = -1), #maximise
lower = c(0.1,0),
upper = c(100,0.99999) ,
method = "L-BFGS-B"
)
lambda_zip_estimate <- mle_zip$par[1]
pi_zip_estimate <- mle_zip$par[2]
aic_zip <- -2*mle_zip$value + 2*(2)
data_frame(`Lamda estimate`= lambda_zip_estimate , `Pi estimate` = pi_zip_estimate,  AIC = aic_zip) %>% knitr::kable()
aics <- c(aic_poisson,aic_negbinom,aic_poismix,aic_zip)
model <- c("Poisson","Negative Binomial","Poisson Mixture","Zero Inflated Poisson")
data_frame(Model = model, AIC = aics) %>% knitr::kable()
#overlay distributions
#generate zip_predictions
# generate_zip_prob <- function(lambda,pi,x){
#
#   if(x == 0){
#     return(pi + (1-pi)*exp(-lambda))
#   }else{
#     return((1-pi)*dpois(x=x,lambda=lambda))
#
#   }
#
# }
generate_negbinom_prob <- function(m,r,x){
(gamma(r + x))/(factorial(x)*gamma(r)) * (m/(m+r))^(x) * (r /(r+m))^(r)
}
probs <- numeric(81)
for (i in 0:80){
probs[i+1] <- generate_negbinom_prob(m = m_estimate , r = r_estimate , x=i)
}
#get expected frequencies
expected_freq <- probs*n
comparison_df <- table(dat$Counts) %>%
as_data_frame %>%
rename(accident_count = Var1,
observed = n) %>%
mutate(expected = expected_freq[1:56],
accident_count = as.numeric(accident_count)) %>%  #take only first 56 values
gather(Type,`Frequency` ,c(observed,expected))
ggplot(comparison_df, aes(x = accident_count , y = Frequency , fill = Type )) +
geom_bar( stat="identity",alpha = .8,
position="dodge") +
labs(x="Accident counts", y="Frequency" , title="Observed vs Expected Frequencies as per Negative Binomial") +
theme_classic() +
theme(plot.title = element_text(hjust=0.5))
#qq plot
#check if any of this is right
fit_nbinom <- fitdist(data = x , method = "mle","nbinom")
# q-q
fitdistrplus::qqcomp(fit_nbinom,addlegend = F, main = "Q-Q plot for Negative Binomial")
chisq.test(filter(comparison_df,Type == "observed")$Frequency,filter(comparison_df,Type != "observed")$Frequency)
#need to calulcate mle at EACH point
m_range <- seq(1,56, by = 0.1) #values of m for profile m loglikelihood
r_range <- seq(0.01,0.8 , by = 0.01)
calc_negbinom_loglik_profile <- function(r,m){
loglik <- -n*log(gamma(r)) + sum(log(gamma(r+x))) - sum(log(factorial(x))) + sum(x) * log(m/(m+r)) + n*r*log(r/(r+m))
return(loglik)
}
profile_likelihood_calc_mean <- function(){
#Calculate profile for M by calculating mle for R at eacg value of M
estimated_r_s <- numeric(length(m_range))
for(i in seq_along(m_range)){
mle_profile <- #find mle for r at each value of m
optim(
par = 1,
m = m_range[i],
fn = calc_negbinom_loglik_profile ,
control = list(fnscale = -1),
lower = 0.1,
upper = 30 ,  # i presume shape cant take a ridiculous value like this
method = "L-BFGS-B"
)
estimated_r_s[i] <- mle_profile$par
}
return(estimated_r_s)
}
estimated_r_values <- profile_likelihood_calc_mean()
#For each r in estimated_r_values, sub in M from 1:56 and trace it
m_profile_loglik_values <- numeric(length(m_range))
for (i in seq_along(estimated_r_values)){
m_profile_loglik_values[i] <- calc_negbinom_loglik_profile(r = estimated_r_values[i], m = m_range[i])
}
data_frame(x = m_range, val = m_profile_loglik_values) %>%
ggplot(aes(x=x,y=val))+
geom_point(size = 1) +
scale_x_continuous(breaks = seq(0,56,by=2)) +
geom_vline(xintercept = 6.9, col = "red", lty = "dashed") +
labs(x = "M parameter value", y = "Profile Loglikelihood" , title = "Profile Loglikelihood of M Parameter") +
theme_classic()
#Now we do the profile loglikelihood for the r
#have to redfine function to allow for correct optimisation
calc_negbinom_loglik_profile <- function(m,r){
loglik <- -n*log(gamma(r)) + sum(log(gamma(r+x))) - sum(log(factorial(x))) + sum(x) * log(m/(m+r)) + n*r*log(r/(r+m))
return(loglik)
}
profile_likelihood_calc_shape <- function(){
#Calculate profile for R by calculating mle for M at eacg value of R
estimated_m_s <- numeric(length(r_range))
for(i in seq_along(r_range)){
mle_profile <- #find mle for r at each value of m
optim(
par = 1,
r = r_range[i],
fn = calc_negbinom_loglik_profile ,
control = list(fnscale = -1),
lower = 1,
upper = 100 ,
method = "L-BFGS-B"
)
estimated_m_s[i] <- mle_profile$par
}
return(estimated_m_s)
}
estimated_m_values <- profile_likelihood_calc_shape() #all m values are the same, this make sense as m is the dude who will afect loglikelihood
r_profile_loglik_values <- numeric(length(r_range))
for (i in seq_along(estimated_m_values)){
r_profile_loglik_values[i] <- calc_negbinom_loglik_profile(m = estimated_m_values[i], r = r_range[i])
}
data_frame(x = r_range, val = r_profile_loglik_values) %>%
ggplot(aes(x=x,y=val))+
geom_point(size = 1) +
scale_x_continuous(breaks = seq(0,0.8,by=0.05)) +
geom_vline(xintercept = 0.58, col = "red", lty = "dashed") +
labs(x = "R shape parameter value", y = "Profile Loglikelihood" , title = "Profile Loglikelihood of R Shape Parameter") +
theme_classic()
