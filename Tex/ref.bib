@InProceedings{aldous,
author="Aldous, David J.",
editor="Hennequin, P. L.",
title="Exchangeability and related topics",
booktitle="{\'E}cole d'{\'E}t{\'e} de Probabilit{\'e}s de Saint-Flour XIII --- 1983",
year="1985",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="1--198",
isbn="978-3-540-39316-0"
}


@inproceedings{bennet2002,
 author = {Bennett, Paul N. and Dumais, Susan T. and Horvitz, Eric},
 title = {Probabilistic Combination of Text Classifiers Using Reliability Indicators: Models and Results},
 booktitle = {Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
 series = {SIGIR '02},
 year = {2002},
 isbn = {1-58113-561-0},
 location = {Tampere, Finland},
 pages = {207--214},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/564376.564413},
 doi = {10.1145/564376.564413},
 acmid = {564413},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {classifier combination, metaclassifiers, reliability indicators, text classification},
} 




@article{zhang2015,
  author    = {Xiang Zhang and
               Junbo Jake Zhao and
               Yann LeCun},
  title     = {Character-level Convolutional Networks for Text Classification},
  journal   = {CoRR},
  volume    = {abs/1509.01626},
  year      = {2015},
  url       = {http://arxiv.org/abs/1509.01626},
  archivePrefix = {arXiv},
  eprint    = {1509.01626},
  timestamp = {Wed, 07 Jun 2017 14:41:26 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/ZhangZL15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@Article{beel2016,
author="Beel, Joeran
and Gipp, Bela
and Langer, Stefan
and Breitinger, Corinna",
title="Research-paper recommender systems: a literature survey",
journal="International Journal on Digital Libraries",
year="2016",
month="Nov",
day="01",
volume="17",
number="4",
pages="305--338",
abstract="In the last 16 years, more than 200 research articles were published about research-paper recommender systems. We reviewed these articles and present some descriptive statistics in this paper, as well as a discussion about the major advancements and shortcomings and an overview of the most common recommendation concepts and approaches. We found that more than half of the recommendation approaches applied content-based filtering (55 {\%}). Collaborative filtering was applied by only 18 {\%} of the reviewed approaches, and graph-based recommendations by 16 {\%}. Other recommendation concepts included stereotyping, item-centric recommendations, and hybrid recommendations. The content-based filtering approaches mainly utilized papers that the users had authored, tagged, browsed, or downloaded. TF-IDF was the most frequently applied weighting scheme. In addition to simple terms, n-grams, topics, and citations were utilized to model users' information needs. Our review revealed some shortcomings of the current research. First, it remains unclear which recommendation concepts and approaches are the most promising. For instance, researchers reported different results on the performance of content-based and collaborative filtering. Sometimes content-based filtering performed better than collaborative filtering and sometimes it performed worse. We identified three potential reasons for the ambiguity of the results. (A) Several evaluations had limitations. They were based on strongly pruned datasets, few participants in user studies, or did not use appropriate baselines. (B) Some authors provided little information about their algorithms, which makes it difficult to re-implement the approaches. Consequently, researchers use different implementations of the same recommendations approaches, which might lead to variations in the results. (C) We speculated that minor variations in datasets, algorithms, or user populations inevitably lead to strong variations in the performance of the approaches. Hence, finding the most promising approaches is a challenge. As a second limitation, we noted that many authors neglected to take into account factors other than accuracy, for example overall user satisfaction. In addition, most approaches (81 {\%}) neglected the user-modeling process and did not infer information automatically but let users provide keywords, text snippets, or a single paper as input. Information on runtime was provided for 10 {\%} of the approaches. Finally, few research papers had an impact on research-paper recommender systems in practice. We also identified a lack of authority and long-term research interest in the field: 73 {\%} of the authors published no more than one paper on research-paper recommender systems, and there was little cooperation among different co-author groups. We concluded that several actions could improve the research landscape: developing a common evaluation framework, agreement on the information to include in research papers, a stronger focus on non-accuracy aspects and user modeling, a platform for researchers to exchange information, and an open-source framework that bundles the available recommendation approaches.",
issn="1432-1300",
doi="10.1007/s00799-015-0156-0",
url="https://doi.org/10.1007/s00799-015-0156-0"
}






@Article{zhou2017,
author="Zhou, Houkui
and Yu, Huimin
and Hu, Roland",
title="Topic evolution based on the probabilistic topic model: a review",
journal="Frontiers of Computer Science",
year="2017",
month="Oct",
day="01",
volume="11",
number="5",
pages="786--802",
abstract="Accurately representing the quantity and characteristics of users' interest in certain topics is an important problem facing topic evolution researchers, particularly as it applies to modern online environments. Search engines can provide information retrieval for a specified topic from archived data, but fail to reflect changes in interest toward the topic over time in a structured way. This paper reviews notable research on topic evolution based on the probabilistic topic model from multiple aspects over the past decade. First, we introduce notations, terminology, and the basic topic model explored in the survey, then we summarize three categories of topic evolution based on the probabilistic topic model: the discrete time topic evolution model, the continuous time topic evolutionmodel, and the online topic evolution model. Next, we describe applications of the topic evolution model and attempt to summarize model generalization performance evaluation and topic evolution evaluation methods, as well as providing comparative experimental results for different models. To conclude the review, we pose some open questions and discuss possible future research directions.",
issn="2095-2236",
doi="10.1007/s11704-016-5442-5",
url="https://doi.org/10.1007/s11704-016-5442-5"
}


@article{zelikman2018,
  author    = {Eric Zelikman},
  title     = {Context is Everything: Finding Meaning Statistically in Semantic Spaces},
  journal   = {CoRR},
  volume    = {abs/1803.08493},
  year      = {2018},
  url       = {http://arxiv.org/abs/1803.08493},
  archivePrefix = {arXiv},
  eprint    = {1803.08493},
  timestamp = {Wed, 11 Apr 2018 11:12:46 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1803-08493},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{joulin2016,
  author    = {Armand Joulin and
               Edouard Grave and
               Piotr Bojanowski and
               Tomas Mikolov},
  title     = {Bag of Tricks for Efficient Text Classification},
  journal   = {CoRR},
  volume    = {abs/1607.01759},
  year      = {2016},
  url       = {http://arxiv.org/abs/1607.01759},
  archivePrefix = {arXiv},
  eprint    = {1607.01759},
  timestamp = {Wed, 07 Jun 2017 14:42:39 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/JoulinGBM16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}





@INPROCEEDINGS{basu2003, 
author={A. Basu and C. Walters and M. Shepherd}, 
booktitle={36th Annual Hawaii International Conference on System Sciences, 2003. Proceedings of the}, 
title={Support vector machines for text categorization}, 
year={2003}, 
volume={}, 
number={}, 
pages={7 pp.-}, 
keywords={classification;information retrieval;neural nets;support vector machines;text analysis;artificial neural network;document retrieval;support vector machine;support vector machines;text categorization;text document sorting;Artificial neural networks;Clustering algorithms;Humans;Machine learning;Machine learning algorithms;Sorting;Support vector machine classification;Support vector machines;Text categorization;Text recognition}, 
doi={10.1109/HICSS.2003.1174243}, 
ISSN={}, 
month={Jan},}

@article{starkweather,
  title={Multinomial logistic regression},
  author={Starkweather, Jon and Moske, Amanda Kay},
  journal={Consulted page at September 10th: http://www. unt. edu/rss/class/Jon/Benchmarks/MLR\_JDS\_Aug2011. pdf},
  volume={29},
  pages={2825--2830},
  year={2011}
}


@article{miao2009,
title = "Rough set based hybrid algorithm for text classification",
journal = "Expert Systems with Applications",
volume = "36",
number = "5",
pages = "9168 - 9174",
year = "2009",
issn = "0957-4174",
doi = "https://doi.org/10.1016/j.eswa.2008.12.026",
url = "http://www.sciencedirect.com/science/article/pii/S0957417408008919",
author = "Duoqian Miao and Qiguo Duan and Hongyun Zhang and Na Jiao",
keywords = "Text classification, Variable precision rough set (VPRS), -nearest neighbor (kNN), Rocchio algorithm"
}

@article{Xu,
author = {Xu, Baoxun and Guo, Xiufeng and Ye, Yunming and Cheng, Jiefeng},
year = {2012},
month = {12},
pages = {},
title = {An Improved Random Forest Classifier for Text Categorization},
volume = {7},
booktitle = {Journal of Computers}
}

}

@article{Wiener,
author = {Liaw, Andy and Wiener, Matthew},
year = {2001},
month = {11},
pages = {},
title = {Classification and Regression by RandomForest},
volume = {23},
booktitle = {Forest}
}

@Article{Cortes1995,
author="Cortes, Corinna
and Vapnik, Vladimir",
title="Support-Vector Networks",
journal="Machine Learning",
year="1995",
month="Sep",
day="01",
volume="20",
number="3",
pages="273--297",
abstract="The support-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.",
issn="1573-0565",
doi="10.1023/A:1022627411411",
url="https://doi.org/10.1023/A:1022627411411"}

@Article{Breiman2001,
author="Breiman, Leo",
title="Random Forests",
journal="Machine Learning",
year="2001",
month="Oct",
day="01",
volume="45",
number="1",
pages="5--32",
abstract="Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund {\&} R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148--156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.",
issn="1573-0565",
doi="10.1023/A:1010933404324",
url="https://doi.org/10.1023/A:1010933404324"
}



@article{CoverT,
 author = {Cover, T. and Hart, P.},
 title = {Nearest Neighbor Pattern Classification},
 journal = {IEEE Trans. Inf. Theor.},
 issue_date = {January 1967},
 volume = {13},
 number = {1},
 month = sep,
 year = {2006},
 issn = {0018-9448},
 pages = {21--27},
 numpages = {7},
 url = {https://doi.org/10.1109/TIT.1967.1053964},
 doi = {10.1109/TIT.1967.1053964},
 acmid = {2267456},
 publisher = {IEEE Press},
 address = {Piscataway, NJ, USA},
} 


@inproceedings{fragos2005,
  title={A Weighted Maximum Entropy Language Model for Text Classification},
  author={Kostas Fragos and Yannis Maistros and Christos Skourlas},
  booktitle={NLUCS},
  year={2005}
}


@article{fragos2014,
title = "Combining Probabilistic Classifiers for Text Classification",
journal = "Procedia - Social and Behavioral Sciences",
volume = "147",
pages = "307 - 312",
year = "2014",
note = "3rd International Conference on Integrated Information (IC-ININFO)",
issn = "1877-0428",
doi = "https://doi.org/10.1016/j.sbspro.2014.07.098",
url = "http://www.sciencedirect.com/science/article/pii/S1877042814040099",
author = "Kostas Fragos and Petros Belsis and Christos Skourlas"
}



@inproceedings{jain2016,
  title={Text Classification by Combining Text Classifiers to Improve the Efficiency of Classification},
  author={Aaditya Jain and Jyoti Mandowara},
  year={2016}
}



@article{isa2008,
  title={Text Document Preprocessing with the Bayes Formula for Classification Using the Support Vector Machine},
  author={Dino Isa and Lam Hong Lee and V. P. Kallimani and Rajprasad Rajkumar},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  year={2008},
  volume={20},
  pages={1264-1272}
}



@article{blei2003,
  title={Latent Dirichlet Allocation},
  author={David M. Blei and Andrew Y. Ng and Michael I. Jordan},
  journal={Journal of Machine Learning Research},
  year={2003},
  volume={3},
  pages={993-1022}
}


@inproceedings{wang2012,
  title={Baselines and Bigrams: Simple, Good Sentiment and Topic Classification},
  author={Sida I. Wang and Christopher D. Manning},
  booktitle={ACL},
  year={2012}
}

@inproceedings{McCallum1998,
  title={A Comparison of Event Models for Naive Bayes Text Classification},
  author={Andrew McCallum and Kamal Nigam},
  booktitle={ACL},
  year={1998}
}



@inproceedings{purver_def,
 author = {Purver, Matthew and Griffiths, Thomas L. and K\"{o}rding, Konrad P. and Tenenbaum, Joshua B.},
 title = {Unsupervised Topic Modelling for Multi-party Spoken Discourse},
 booktitle = {Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics},
 series = {ACL-44},
 year = {2006},
 location = {Sydney, Australia},
 pages = {17--24},
 numpages = {8},
 url = {https://doi.org/10.3115/1220175.1220178},
 doi = {10.3115/1220175.1220178},
 acmid = {1220178},
 publisher = {Association for Computational Linguistics},
 address = {Stroudsburg, PA, USA},
} 