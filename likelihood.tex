\documentclass[11pt,preprint, authoryear]{elsarticle}

\usepackage{lmodern}
%%%% My spacing
\usepackage{setspace}
\setstretch{1.2}
\DeclareMathSizes{12}{14}{10}{10}

% Wrap around which gives all figures included the [H] command, or places it "here". This can be tedious to code in Rmarkdown.
\usepackage{float}
\let\origfigure\figure
\let\endorigfigure\endfigure
\renewenvironment{figure}[1][2] {
    \expandafter\origfigure\expandafter[H]
} {
    \endorigfigure
}

\let\origtable\table
\let\endorigtable\endtable
\renewenvironment{table}[1][2] {
    \expandafter\origtable\expandafter[H]
} {
    \endorigtable
}


\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
    \usepackage{xltxtra,xunicode}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
  \newcommand{\euro}{â‚¬}
\fi

\usepackage{amssymb, amsmath, amsthm, amsfonts}

\usepackage[round]{natbib}
\bibliographystyle{natbib}
\def\bibsection{\section*{References}} %%% Make "References" appear before bibliography
\usepackage{longtable}
\usepackage[margin=2.3cm,bottom=2cm,top=2.5cm, includefoot]{geometry}
\usepackage{fancyhdr}
\usepackage[bottom, hang, flushmargin]{footmisc}
\usepackage{graphicx}
\numberwithin{equation}{section}
\numberwithin{figure}{section}
\numberwithin{table}{section}
\setlength{\parindent}{0cm}
\setlength{\parskip}{1.3ex plus 0.5ex minus 0.3ex}
\usepackage{textcomp}
\renewcommand{\headrulewidth}{0.2pt}
\renewcommand{\footrulewidth}{0.3pt}

\usepackage{array}
\newcolumntype{x}[1]{>{\centering\arraybackslash\hspace{0pt}}p{#1}}

%%%%  Remove the "preprint submitted to" part. Don't worry about this either, it just looks better without it:
\makeatletter
\def\ps@pprintTitle{%
  \let\@oddhead\@empty
  \let\@evenhead\@empty
  \let\@oddfoot\@empty
  \let\@evenfoot\@oddfoot
}
\makeatother

 \def\tightlist{} % This allows for subbullets!

\usepackage{hyperref}
\hypersetup{breaklinks=true,
            bookmarks=true,
            colorlinks=true,
            citecolor=blue,
            urlcolor=blue,
            linkcolor=blue,
            pdfborder={0 0 0}}


% The following packages allow huxtable to work:
\usepackage{siunitx}
\usepackage{multirow}
\usepackage{hhline}
\usepackage{calc}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{colortbl}

\urlstyle{same}  % don't use monospace font for urls
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\setcounter{secnumdepth}{5}

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}

%%% Include extra packages specified by user
% Insert custom packages here as follows
% \usepackage{tikz}

%%% Hard setting column skips for reports - this ensures greater consistency and control over the length settings in the document.
%% page layout
%% paragraphs
\setlength{\baselineskip}{12pt plus 0pt minus 0pt}
\setlength{\parskip}{12pt plus 0pt minus 0pt}
\setlength{\parindent}{0pt plus 0pt minus 0pt}
%% floats
\setlength{\floatsep}{12pt plus 0 pt minus 0pt}
\setlength{\textfloatsep}{20pt plus 0pt minus 0pt}
\setlength{\intextsep}{14pt plus 0pt minus 0pt}
\setlength{\dbltextfloatsep}{20pt plus 0pt minus 0pt}
\setlength{\dblfloatsep}{14pt plus 0pt minus 0pt}
%% maths
\setlength{\abovedisplayskip}{12pt plus 0pt minus 0pt}
\setlength{\belowdisplayskip}{12pt plus 0pt minus 0pt}
%% lists
\setlength{\topsep}{10pt plus 0pt minus 0pt}
\setlength{\partopsep}{3pt plus 0pt minus 0pt}
\setlength{\itemsep}{5pt plus 0pt minus 0pt}
\setlength{\labelsep}{8mm plus 0mm minus 0mm}
\setlength{\parsep}{\the\parskip}
\setlength{\listparindent}{\the\parindent}
%% verbatim
\setlength{\fboxsep}{5pt plus 0pt minus 0pt}



\begin{document}

\begin{frontmatter}  %

\title{Theory of Statistics Likelihood Assigment}

\author[Add1]{Sean Soutar STRSEA001}
\ead{sean.soutar@gmail.com}

\author[Add2]{Fabio Fehr FHRFAB001}
\ead{FHRFAB001@myuct.ac.za}




\address[Add1]{UCT Statistics Honours, Cape Town, South Africa}
\address[Add2]{UCT Statistics Honours, Cape Town, South Africa}


\begin{abstract}
\small{
This project will explore the Accidents dataset. Various count models
such as Poisson, Negative Binomial, Mixture of 2 Poissons and Zero
Inflated Poisson models will be applied to the data. The model with the
strongest support will be chosen and discussed. Profile likelihoods and
confidence intervals for the parameters will be found and displayed for
the chosen model.
}
\end{abstract}

\vspace{1cm}

\begin{keyword}
\footnotesize{
Likelihood \sep Overdispersion \sep Count data \\ \vspace{0.3cm}
\textit{JEL classification} 
}
\end{keyword}
\vspace{0.5cm}
\end{frontmatter}



%________________________
% Header and Footers
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagestyle{fancy}
\chead{}
\rhead{}
\lfoot{}
\rfoot{\footnotesize Page \thepage\\}
\lhead{}
%\rfoot{\footnotesize Page \thepage\ } % "e.g. Page 2"
\cfoot{}

%\setlength\headheight{30pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%________________________

\headsep 35pt % So that header does not go over title




\section{Introduction}\label{introduction}

This assignment is an explorative report on a dataset containing the
number of accidents on two-lane (same direction) road segments in Cape
Town over a five-year period. The segments differ in length between 0.2
and 7.2 km. The aim of the report is to find and fit a model which
accurately describes the accident dataset. This report will first
explore the data then fit different count distributions. The best
fitting model will then be chosen. Once a model has been selected, the
profile likelihood and confidence intervals for the model parameters
will be calculated. The results will then be analysed critically and
conclusions will be made whilst suggesting further areas of
investigation.

\subsection{Exploratory data analysis}\label{exploratory-data-analysis}

To better understand our data this report shall explore the following
properties; Firstly we examine the type of data within the accidents
dataset and discuss whether our data is discrete ordinal or continuous.
After the symmetry of the data and bounds will be discussed. This leads
the exploration to outliers and extreme values.

\includegraphics{likelihood_files/figure-latex/unnamed-chunk-1-1.pdf}

\subsubsection{\texorpdfstring{Data type
\label{data_description}}{Data type }}\label{data-type}

There are many instances where zero accidents were observed. This
accounts for approximately 25.18\% of the data. This suggests that the
zero-inflated Poisson should be considered as this proportion is much
higher than what would be expected of a regular Poisson distribution.
The accident counts are discrete random variables. Specifically, they
are discrete positive definite random variables on the interval
\(R \in \{0;+ \infty\}\). Summary statistics of the data are shown
below.

\begin{longtable}[]{@{}rrr@{}}
\caption{Summary statisitics}\tabularnewline
\toprule
Mean & Variance & Median\tabularnewline
\midrule
\endfirsthead
\toprule
Mean & Variance & Median\tabularnewline
\midrule
\endhead
6.9179 & 85.0858 & 4\tabularnewline
\bottomrule
\end{longtable}

In the Poisson distribution, the mean should equal the variance. The
sample variance far exceeds the sample mean. This indicates
overdispersion if the Poisson distribution were to be used. This is when
the observations are more variable than what would be expected. This
suggests that alternative count models and mixture distributions should
be used.

\subsubsection{Symmetry}\label{symmetry}

This property is visually seen in the histogram and boxplot. All counts
are greater than zero with a median value of 4 accidents. The largest
accident observed is 70 accidents. THe histogram shows that the data are
non-symetrical and positively skewed which is usually expected of count
data.

\subsubsection{Outliers}\label{outliers}

From the boxplot it clear that many outliers exist. One common method of
classifying a point as an extreme value or outlier is if it falls more
than 1.5 times the inner-quartile range above the upper quartile. The
proportion of outliers within our data set amount to 15.26\%.

\section{Methods}\label{methods}

\subsection{Model Formulation}\label{model-formulation}

The data is discrete, asymmetric, positive definite, contains many
positive outliers and many zeros. This would suggest distributions such
as Poisson, Negative Binomial, mixture distribution of 2 Poissons and a
zero inflated Poisson. For all optimisation we shall constrain the
bounds in order to ensure valid regions for our parameters. The best
fitting distribution will be reparametised to aid interpretation and
make the likelihood overal more quadratic.

\subsection{Akiake Information Coefficient
(AIC)}\label{akiake-information-coefficient-aic}

The AIC metric can be used to compare models from different families of
distributions. They can be used to compare relative goodness of fit
between models. Overfitting the model with too many parameter is
penalised by an increased AIC, thus a lower AIC value indicates a better
fitting model.

\begin{align*}
\text{AIC} &= -2l(\hat{\theta}) + 2\text{p} \\
p &= \text{Number of estimated parameters}
\end{align*}

\subsection{Bayesian Information Criterion
(BIC)}\label{bayesian-information-criterion-bic}

This metric, like AIC, also compares relative goodness of fit between
models but penalises complex models when the sample size is large. BIC
tends to produce simpler models than AIC.

\begin{align*}
\text{BIC} &= -2l(\hat{\theta}) + log(n) \\
n &= \text{Number of observations}
\end{align*}

\subsection{Model Distributions}\label{model-distributions}

\subsubsection{Poisson}\label{poisson}

The Poisson is a discrete probability distribution that expresses the
probability of a given number of events occurring in a fixed interval of
time if these events occur with a known constant rate and independently
of the time since the last event.This is typically used in count data
where your mean and variance are equal.

\begin{align*} 
p(x) & =  \frac{e^{-\lambda} \lambda^x}{x!},\ \ x\in \{0,1,\ldots,\infty\},\lambda>0 \\
\\
L(\lambda|x) & = \prod_{i=1}^n p(x_i) \\
\\
L(\lambda|x) & =\dfrac{e^{-n\lambda}\lambda^{\sum_{i=1}^n x_i}}{\prod_{i=1}^n x_i!}\\
\\
l(\lambda|x) & =-n\lambda +  \left(\sum_{i=1}^n x_i\right)\ln \lambda - \sum_{i=1}^{n}\ln(x_i!)
\end{align*}

The Poisson is characterised by the \(\lambda\) parameter which denotes
the population average rate of event occurence. In this context it would
be the average number of accidents per unit time frame. Due to the
variance being far higher than our mean in our data we would expect that
a Poisson distribution would not fit very well.

\begin{longtable}[]{@{}rrr@{}}
\caption{Poisson MLE's \& information metrics}\tabularnewline
\toprule
\(\hat{\lambda}\) & AIC & BIC\tabularnewline
\midrule
\endfirsthead
\toprule
\(\hat{\lambda}\) & AIC & BIC\tabularnewline
\midrule
\endhead
6.9179 & 20263.64 & 20269.04\tabularnewline
\bottomrule
\end{longtable}

\subsubsection{Negative Binomial}\label{negative-binomial}

The Negative Binomial distribution is a discrete probability function of
the number of successes in a sequence of independant and identitically
distributed Bernoulli trials. The parameters \(p\) \& \(r\) measure the
probability of success in an individual trial and the number of
successes until \(r\) failures occure. The mean in a negative binomial
is defined as \(m={\frac{pr}{1-p}}\) thus we can reparamatize the
distribution in terms of the mean parameter m and shape parameter \(r\)
such that \(p={\frac{m}{m+r}}\) and \(1-p={\frac{r}{m+r}}\). We can also
manipulate the constant term as follows.

\begin{align*}
\binom{x+r-1}{x} &= {\frac {(x+r-1)(x+r-2)\dotsm (r)}{x!}} = {\frac {\Gamma(x+r)}{x!\,\Gamma (r)}}
\end{align*}

This gives us the negative binomial in the form

\begin{align*} 
p(x) & =  {\frac {\Gamma (r+x)}{x!\,\Gamma (r)}}\left({\frac {m}{r+m}}\right)^{x}\left({\frac {r}{r+m}}\right)^{r}\quad {\text{for }}x=0,1,2,\dotsc \\
\\
L(m,r|x) & = \prod_{i=1}^n p(x_i) \\
\\
L(m,r|x) & ={[\frac{1}{\Gamma (r)}]}^{n} \prod_{i=1}^{n}{\frac{\Gamma (r+x_i)}{x_{i}!}} (\frac{m}{r + m})^{\Sigma_{i=1}^n x_i} (\frac{r}{r + m})^{nr}   \\
\\
l(m,r|x) & = -n\ln[\Gamma (r)] + \sum^{n}_{i=1} \ln(\Gamma (r + x_i)) -\sum^{n}_{i=1}\ln x_i! + \sum^{n}_{i=1} x_{i} \ln (\frac{m}{r + m}) + nr \ln (\frac{r}{r + m})
\end{align*}

It is important to note that the variance of a Negative Binomial under
this parameterisation is \(m + \frac{m^2}{r}\) and always larger than
our mean \(m\). This would suggest a better fit than our Poisson model.
Shape parameters are often regarded as nuisance parameters and do not
play a meaningful role in maximising likelihood. Therefore, since we
desire no under or over dispersion, we can express the shape parameter
as a function of the mean parameter to be esimated and the sample
variance.

\begin{align*}
Var(x) &= m + \frac{m^2}{r} \\
\\
r &= \frac{m^2}{Var(x) - m} \\
\\
\hat{r} &= \frac{\hat{m}^2}{S^2 - \hat{m}}
\end{align*}

\begin{longtable}[]{@{}rrrr@{}}
\caption{Negative Binomial MLE's \& information metrics}\tabularnewline
\toprule
Mean \(\hat{m}\) & Shape \(\hat{r}\) & AIC & BIC\tabularnewline
\midrule
\endfirsthead
\toprule
Mean \(\hat{m}\) & Shape \(\hat{r}\) & AIC & BIC\tabularnewline
\midrule
\endhead
6.8108 & 0.5926 & 9626.92 & 9630.31\tabularnewline
\bottomrule
\end{longtable}

\subsubsection{Mixture of 2 Poissons}\label{mixture-of-2-poissons}

A finite mixture distributon of two Poisson variables will now be
explored. A possibly reason for the overdispersion is that the data are
from two separate Poisson distributions. Since it is not known from
which distribution that any given data point is from, presuming that the
two distribution mixture is appropriate, an additional mixing proportion
parameter \(p\) needs to be estimated.

\begin{align*} 
p(x|\lambda_1,\lambda_2,p) & =  p\frac{e^{-\lambda_1} \lambda_1^x}{x!} + (1-p)\frac{e^{-\lambda_2} \lambda_2^x}{x!},\ \ x\in \{0,1,\ldots,\infty\},\lambda_1 , \lambda_2 , p >0 \\
\\
L(\lambda_1,\lambda_2,p|x) & = \prod_{i=1}^n p(x_i) \\
\\
L(\lambda_1,\lambda_2,p|x) & =  \prod_{i=1}^n p\frac{e^{-\lambda_1} \lambda_1^{x_i}}{x_i!} + (1-p)\frac{e^{-\lambda_2} \lambda_2^{x_i}}{x_i!} \\
\\
l(\lambda_1,\lambda_2,p|x) & \sum^n_{i=1} \ln [ p\frac{e^{-\lambda_1} \lambda_1^{x_i}}{x_i!} + (1-p)\frac{e^{-\lambda_2} \lambda_2^{x_i}}{x_i!} ]
\end{align*}

The parameters \(\lambda_1\) and \(\lambda_2\) refer to the average
number of road accidents per road stretch for the first and second
distribution respectively. The parameter \(p\) is the proportion
parameter. This represents the probability that a given observation
belongs to distribution 1. Therefore, the probability that an
observation belongs to distribution 2 is the \(1-p\).

\begin{longtable}[]{@{}rrrrr@{}}
\caption{Poisson-Poisson MLE's \& information metrics}\tabularnewline
\toprule
\(\hat{\lambda_1}\) & \(\hat{\lambda_2}\) & Proportion p & AIC &
BIC\tabularnewline
\midrule
\endfirsthead
\toprule
\(\hat{\lambda_1}\) & \(\hat{\lambda_2}\) & Proportion p & AIC &
BIC\tabularnewline
\midrule
\endhead
19.3809 & 2.8407 & 0.2465 & 12075.12 & 12076.52\tabularnewline
\bottomrule
\end{longtable}

\subsubsection{Zero inflated Poisson}\label{zero-inflated-poisson}

The Zero Inflated Poisson is also a finite mixture distribution. This
model supposes that the data can come from two distributions. The one is
a Zero Process and the other is a Poisson process that can only take on
non-zero values. This model is useful if there are many zeroes in the
data. This was seen to be the case as discussed in
\ref{data_description}. The Zero Inflated Poisson is a piecewise defined
distribution with different mass functions for predicting the
probability that a given observation will be zero rather than non-zero.

\begin{align*}
p(x_{i}=0) &= \pi +(1-\pi )e^{{-\lambda }} \\
\\
p(x_{i} \neq 0) &=(1-\pi ){\frac  {\lambda ^{{x_{i}}}e^{{-\lambda }}}{x_{i}!}},\qquad x_{i}\geq 1 \\
\\
L(\lambda,\pi|x) &= L(\lambda,\pi |x = 0)L(\lambda,\pi|x \neq 0)
\end{align*}

An indicator variable \(I\) is defined. \[ 
I =\begin{cases} 
      0 & x=0 \\
      1 & x \neq 0
   \end{cases}
\]

\begin{align*}
L(\lambda,\pi|x) &= \prod^{n}_{i=1} p(x_{i}=0)^{1 - I}p(x_{i} \neq 0)^{I} \\
\\
L(\lambda,\pi|x) &= \prod^{n}_{i=1} [\pi +(1-\pi )e^{{-\lambda }}]^{1 - I}[(1-\pi ){\frac  {\lambda ^{{x_{i}}}e^{{-\lambda }}}{x_{i}!}}]^{I} \\
\\
l(\lambda,\pi|x) &= \sum^{n}_{i=1} \ln[(1-I)[\pi +(1-\pi )e^{{-\lambda }}] + I[(1-\pi ){\frac  {\lambda ^{{x_{i}}}e^{{-\lambda }}}{x_{i}!}}]]
\end{align*}

The parameter \(\lambda\) is the average rate of accidents per road
stretch. The parameter \(\pi\) is the probability of additional zeroes
observed in our data. This mixture distribution has a mean of
\((1-\pi)\lambda\) and variance \((1-\pi)\lambda(1+\pi\lambda)\). It is
clear that the variation in this distribution is always greater than the
average thus a good potential model to consider.

\begin{longtable}[]{@{}rrrr@{}}
\caption{Zero inflated Poisson MLE's \& information
metrics}\tabularnewline
\toprule
\(\hat{\lambda}\) & \(\hat{\pi}\) & AIC & BIC\tabularnewline
\midrule
\endfirsthead
\toprule
\(\hat{\lambda}\) & \(\hat{\pi}\) & AIC & BIC\tabularnewline
\midrule
\endhead
9.2456 & 0.2518 & 15556.16 & 15559.56\tabularnewline
\bottomrule
\end{longtable}

\subsection{Model Selection}\label{model-selection}

The AIC and BIC results for each model are summarised below.

\begin{table}

\caption{\label{tab:aic_summary}Models fitted to accident data}
\centering
\begin{tabular}[t]{lrr}
\toprule
Model & AIC & BIC\\
\midrule
Poisson & 20263.64 & 20269.04\\
\textbf{Negative Binomial} & \textbf{9626.92} & \textbf{9630.31}\\
Poisson Mixture & 12075.12 & 12076.52\\
Zero Inflated Poisson & 15556.16 & 15559.56\\
\bottomrule
\end{tabular}
\end{table}

The Negative Binomial has the lowest AIC and BIC value at 9626.92 and
9630.31 respectively. This implies that the Negative Binomial is the
best fitting model when compared to the others. The goodness of fit will
now be assessed further. We start by examining the likelihood surface.
Since the \(r\) shape parameter is a function of the \(m\) parameter for
each \(m\) there is an exact \(r\) meaning that these parameters are
highly correlated.

\includegraphics{likelihood_files/figure-latex/NegBin Likelihood vis-1.pdf}
\includegraphics{likelihood_files/figure-latex/NegBin Likelihood vis-2.pdf}

\begin{verbatim}
## NULL
\end{verbatim}

\begin{verbatim}
##               [,1]         [,2]          [,3]          [,4]
## [1,]  6.766185e-02 0.0208701991 -7.788864e-02  7.788864e-02
## [2,] -1.547565e+00 0.3360922734 -1.254313e+00  1.254313e+00
## [3,]  9.123207e-21 0.0001878758  5.034116e-05 -5.034116e-05
## [4,]  7.107063e-02 4.6950465230  8.143787e-02  9.185621e-01
\end{verbatim}

\subsubsection{Goodness of Fit}\label{goodness-of-fit}

We will now access the goodness of fit by first comparing the observed
accident counts against the expected expected counts given a negative
binomial distribution. This will then be used in the Pearson's
Chi-Squared test and finally plotted using a quantile-quantile plot for
the Negative Binomial.

\includegraphics{likelihood_files/figure-latex/best_model_fit-1.pdf}

As we can see the observed and expected accident counts seem to fit
nicely except for first 5 counts where there is some dissanance. This
could be due to the excess of zeros in the data. We will now construct a
Pearson's Chi-Squared test with the following hypotheses.

\begin{align*}
H_0: & \text{The data is consistent with the Negative Binomial distribution.}\\
H_1: & \text{The data is NOT consistent with the Negative Binomial distribution.} 
\end{align*}

\begin{longtable}[]{@{}rrr@{}}
\caption{Pearson's \(\chi^2\) Goodness of Fit test}\tabularnewline
\toprule
\(\chi^2\) Statistic & \(\chi^2\) DoF & P-Value\tabularnewline
\midrule
\endfirsthead
\toprule
\(\chi^2\) Statistic & \(\chi^2\) DoF & P-Value\tabularnewline
\midrule
\endhead
1456 & 1430 & 0.31\tabularnewline
\bottomrule
\end{longtable}

The goodness of fit test results in a significantly large p-value
meaning that there is evidence to conclude that the data is consistant
with the Negative Binomial distribution. We will now compare our sample
distribution against a Negative Binomial in the form a a
Quantile-Quantile plot.

\includegraphics{likelihood_files/figure-latex/qq_plot-1.pdf}

The plot is in agreement with the prior too goodness of fit evaluations
thus it is reasonable to assume that our data follows a Negative
Binomial distribution. We will now explore the parameters of our
distribution.

\subsection{Profile Likelihood \& Confidence
Intervals}\label{profile-likelihood-confidence-intervals}

The profile likelihood is a techinque used to estimate the likelihood
function of a single parameter when multiple parameters are estimated
simultaenously. The profile likelihoods for the m and r parameters are
calculated as follows:

\begin{align*}
\end{align*}

\includegraphics{likelihood_files/figure-latex/profile_likelihood_m-1.pdf}

\includegraphics{likelihood_files/figure-latex/profile_likelihood_r-1.pdf}

It can be seen that the profile likelihood for the \(m\) parameter is
somewhat quadratic around the MLE, but this is clearly not the case for
the \(r\) shape parameter. Therefore it would be inappropriate to use a
Wilks Likelihood interval or Wald interval for the shape parameter. The
direct likelihood interval should be used. However, the quadracity of
the \(m\) parameter profile likelihood can be further visualised through
the score of the parameter.

\includegraphics{likelihood_files/figure-latex/unnamed-chunk-5-1.pdf}

As we can see, our score function for \(m\) is linear meaning that our
likelihood must be quadratic, further implying that the asymptotic
intervals relying on quadracity could be used.

\subsubsection{Wald Interval}\label{wald-interval}

We can attain an interval for out Maximum Likelihood Estimates if we
assume they have asymptotic normal distribution. This means
\(n\to\infty\) the estimates will be approximately normal with the
following parameters.

\begin{align*}
\hat{m} & \stackrel{.}{\sim} N(m,I(\hat{m})^{-1})
\end{align*}

This means that we can form asymptotic confidence intervals for \(m\)
and \(r\) know as a Wald interval. The advantages of this is it is
simple to calculate provided you have a standard error estimate. Since
the profile likelihood of the shape parameter r is not quadratic around
the MLE, we shall avoid this type of interval and the Wilks Likelihood
ratio interval. The direct likelihood interval shall be used instead.

\begin{longtable}[]{@{}rr@{}}
\caption{Wald interval for m parameter}\tabularnewline
\toprule
Lower bound & Upper bound\tabularnewline
\midrule
\endfirsthead
\toprule
Lower bound & Upper bound\tabularnewline
\midrule
\endhead
6.584562 & 7.037038\tabularnewline
\bottomrule
\end{longtable}

\subsubsection{Wilks Likelihood ratio}\label{wilks-likelihood-ratio}

The Wilks Likelihood Ratio statistic is based on the deviance and is
used to compare a certain parameter against the Maximum Likelihood
estimate for that parameter. If the data come from a Normal distribution
the following result is true, but if the likelihood is quadratic the
following is asymptotically true.

\[W = -2\text{log}R(\theta) \sim \chi^2_p\]

\begin{longtable}[]{@{}rr@{}}
\caption{Wilks Likelihood interval for m parameter}\tabularnewline
\toprule
Lower bound & Upper bound\tabularnewline
\midrule
\endfirsthead
\toprule
Lower bound & Upper bound\tabularnewline
\midrule
\endhead
6.586151 & 7.038553\tabularnewline
\bottomrule
\end{longtable}

\subsubsection{Pure Likelihood Interval}\label{pure-likelihood-interval}

The likelihood interval can be found as \(R(\theta) > \gamma^p\), where
\(\gamma\) can be based on a \(\chi^2\) approximation and \(p\) is the
dimension of \(\theta\). Equivalently we can use the deviance:

\[ -2[\ell(\theta_p) - \ell(\hat{\theta_p})] \sim \chi^2_p. \] We solve
for points of \(\theta\) where the deviance equals the 95th percentile
of \(\chi^2_p\).

\includegraphics{likelihood_files/figure-latex/unnamed-chunk-6-1.pdf}

\begin{longtable}[]{@{}rr@{}}
\caption{interval for m parameter}\tabularnewline
\toprule
Lower bound & Upper bound\tabularnewline
\midrule
\endfirsthead
\toprule
Lower bound & Upper bound\tabularnewline
\midrule
\endhead
6.587527 & 7.03714\tabularnewline
\bottomrule
\end{longtable}

\section{Results}\label{results}

\begin{table}

\caption{\label{tab:intervals}Intervals for m parameter}
\centering
\begin{tabular}[t]{lrr}
\toprule
Intervals & Lower Bound & Upper Bound\\
\midrule
Wald Interval & 6.584562 & 7.037038\\
Wilks Likelihood & 6.586151 & 7.038553\\
Direct Likelihood 15\% & 6.587527 & 7.037140\\
\bottomrule
\end{tabular}
\end{table}

\section{Conclusion}\label{conclusion}

-What are the next steps and how can we improve the models

\section{References}\label{references}

% Force include bibliography in my chosen format:
\newpage
\nocite{*}
\bibliography{}





\end{document}
