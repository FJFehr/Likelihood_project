---
# IMPORTANT: Change settings here, but DO NOT change the spacing. 
# Remove comments and add values where applicable. 
# The descriptions below should be self-explanatory

title: "Theory of Statistics \n Likelihood Assigment"
#subtitle: "This will appear as Right Header"

documentclass: "elsarticle"

# Comment: ----- Follow this pattern for up to 5 authors
Author1: "Sean Soutar STRSEA001"  # First Author
Ref1: "UCT Statistics Honours, Cape Town, South Africa" # First Author's Affiliation
Email1: "sean.soutar\\@gmail.com" # First Author's Email address

Author2: "Fabio Fehr FHRFAB001"
Ref2: "UCT Statistics Honours, Cape Town, South Africa"
Email2: "FHRFAB001\\@myuct.ac.za"
CommonAffiliation_12: FALSE # If Author 1 and 2 have a common affiliation. Works with _13, _23, etc.

#Author3: "John Doe"
#Email3: "JohnSmith\\@gmail.com"

#CorrespAuthor_1: TRUE  # If corresponding author is author 3, e.g., use CorrespAuthor_3: TRUE

keywords: "Likelihood \\sep Overdispersion \\sep Soek" # Use \\sep to separate
JELCodes: ""

# Comment: ----- Manage headers and footers:
#BottomLFooter: $Title$
#BottomCFooter:
#TopLHeader: \leftmark # Adds section name at topleft. Remove comment to add it.
BottomRFooter: "\\footnotesize Page \\thepage\\" # Add a '#' before this line to remove footer.
addtoprule: TRUE
addfootrule: TRUE               # Use if footers added. Add '#' to remove line.

# Setting page margins:
margin: 2.3 # Sides
bottom: 2 # bottom
top: 2.5 # Top

HardSet: TRUE # Hard-set the spacing of words in your document. This will stop LaTeX squashong text to fit on pages, e.g. This is done by hard-setting the spacing dimensions. Set to FALSE if you want LaTeX to optimize this for your paper. 
bibliography: Tex/ref.bib       # Do not edit: Keep this naming convention and location.
RemovePreprintSubmittedTo: TRUE  # Removes the 'preprint submitted to...' at bottom of titlepage
Journal: "Journal of Finance"   # Journal that the paper will be submitting to, if RemovePreprintSubmittedTo is set to TRUE.
toc: no                         # Add a table of contents
numbersections: yes             # Should sections (and thus figures and tables) be numbered?
fontsize: 11pt                  # Set fontsize
linestretch: 1.2                # Set distance between lines.
link-citations: TRUE            # This creates dynamic links to the papers in reference list.
output:
  pdf_document:
    keep_tex: TRUE
    template: Tex/TexDefault.txt
    fig_width: 3.5 # Adjust default figure sizes. This can also be done in the chunks of the text.
    fig_height: 3.5
    include:
      in_header: Tex/packages.txt # Reference file with extra packages
abstract: |
  This project will explore the Accidents dataset and try fit a Poisson, Negative Binomial, Mixture of 2 Poissons and zero inflated Poisson models to the data. The model with the strongest support will be chosen and discussed. Profile likelihoods and confidence intervals for the parameters will be found and displayed of the chosen model.
---

<!-- First: Set your default preferences for chunk options: -->

<!-- If you want a chunk's code to be printed, set echo = TRUE. message = FALSE stops R printing ugly package loading details in your final paper too. I also suggest setting warning = FALSE and checking for warnings in R, else you might find ugly warnings in your paper. -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, fig.width = 6, fig.height = 5, fig.pos="H", fig.pos = 'H')
# Note: Include = FALSE implies the code is executed, but not printed in your pdf.
# warning and message = FALSE implies ugly messages and warnings are removed from your pdf. These should be picked up when you execute the command chunks (code sections below) in your rmd, not printed in your paper!

library(knitr)
library(ggplot2)
library(kableExtra)
library(tidyverse)
library(cowplot)
# 
# devtools::install_version("rmarkdown", version = "1.8", repos = "http://cran.us.r-project.org")


dat <- read.csv("data/accidents.csv") %>% 
  setNames(c("Index","Counts")) %>% 
  as_data_frame() %>% 
  mutate(Counts = as.numeric(Counts))

#define usefule variables
x <- dat$Counts
n <- nrow(dat)

```

#Introduction

This assignment is an explorative report on a dataset containing accident counts. The aim of the report is to find and fit a model which accurately describes the accident dataset. This report will first explore the data then fit different adequate distributions and choose the most appropriate one. Once a model has been selected the profile likelihood and confidence intervals will be programmed and calculated from from first principles. The results will then be analysed critically and conclusions will be made and consider further considerations in the study.

##Exploratory data analysis

To better understand our data this report shall explore the following properties; Firstly we examine the type of data within the accidents dataset and discuss whether our data is discrete ordinal or continuous. After the symmetry of the data and bounds will be discussed. This leads the exploration to outliers and extreme values.

```{r fig.height = 6}

#first column is just an index, second is a  1632 observations

#first visualise the data


  distribution_plot <- ggplot(dat, aes(Counts)) +
  stat_count( fill="blue",
                 alpha = .4,
                 position="dodge")+
  labs(x="Accident counts", y="Frequency" , title="Histogram\n Accident counts") +
  theme_classic() +
  theme(plot.title = element_text(hjust=0.5))

firstbox <- ggplot(dat)+
  geom_boxplot(aes(y= dat$Counts,x=1),
               fill = "cornflowerblue", 
               outlier.color ="firebrick")+
  theme_classic() +
  theme(plot.title = element_text(hjust=0.5))+
  labs(title="Boxplot\n Accident counts") +
  labs(y="Accident counts",x ="")+
  coord_flip()

plot_grid(distribution_plot,firstbox,ncol = 1)

percentage_zero <- ((sum(dat$Counts==0)/nrow(dat))*100) %>% round(digits = 2)

```

### Data type
There are many instances where zero accidents were observed. This accounts for approximately `r percentage_zero`% of the data. This suggests that the zero-inflated Poisson should be considered as this proportion is much higher than what would be expected of a regular Poisson distribution. The accident counts are discrete random variables. Specifically, they are discrete positive definite random variables on the interval $R \in \{0;+ \infty\}$. 
Summary statistics of the data are shown below.

```{r summary_stat , fig.cap = "Summary Statistics of accident data \\label{summary_stats}"}

summarize(.data = dat, Mean= mean(Counts), Variance = var(Counts), Median = median(Counts)) %>% knitr::kable()

```

In the Poisson distribution, the mean should equal the variance. The sample variance far exceeds the sample mean. This indicates overdispersion if the Poisson distribution were to be used. This is when the observations are more variable than what would be expected. This suggests that alternative count models and mixture distributions should be used. 


### Symmetry
This property is visually seen in the histogram and boxplot. All counts are greater than zero with a median value of 4 accidents. The largest accident observed is 70 accidents. THe histogram shows that the data are non-symetrical and positively skewed which is usually expected of count data.

### Outliers
From the boxplot it clear that many outliers exist. One common method of classifying a point as an extreme value or outlier is if it falls more than 1.5 times the inner-quartile range above the upper quartile. The proportion of outliers within our data set amount to 15.26%.

```{r}
#five number summary of the data
five_sum <- summary(dat)

#percentage outliers - 3rd quartile is 9
perc_outlier <- round(length(dat$Counts[dat$Counts > 1.5*9])/length(dat$Counts),4)*100
```


#Methods 

##Model Formulation

The data is discrete, asymmetric, positive definite, contains many positive outliers and many zeros. This would suggest distributions such as Poisson, Negative Binomial, mixture distribution of 2 Poissons and a zero inflated Poisson. 

## Akiake Information Coefficient (AIC)
should we look at biC?
The AIC metric can be used to compare models from different families of distributions. They can be used to compare relative goodness of fit between models. A lower AIC value indicates a better fitting model.


$\text{AIC} = -2l(\hat{\theta}) + 2\text{p} \\
p = \text{Number of estimated parameters}$


###Poisson 

\begin{align*} 
p(x) & =  \frac{e^{-\lambda} \lambda^x}{x!},\ \ x\in \{0,1,\ldots,\infty\},\lambda>0 \\
\\
L(\lambda|x) & = \prod_{i=1}^n p(x_i) \\
\\
L(\lambda|x) & =\dfrac{e^{-n\lambda}\lambda^{\sum_{i=1}^n x_i}}{\prod_{i=1}^n x_i!}\\
\\
l(\lambda|x) & =-n\lambda +  \left(\sum_{i=1}^n x_i\right)\ln \lambda - \sum_{i=1}^{n}\ln(x_i!)
\end{align*}

The Poisson is characterised by the $\lambda$ parameter which denotes the population average rate of event occurence. In this context it would be the average number of accidents per unit time frame.

```{r poisson_fit}

#we can use mle to fit this
mle_lambda <- sum(x)/n

loglik_poisson <- -n*mle_lambda + sum(x)*log(mle_lambda)  -sum(log(factorial(x)))

aic_poisson <- -2*loglik_poisson + 2*(1)

data_frame(`Lambda Estimate`= mle_lambda , AIC = aic_poisson) %>% knitr::kable()


```


###Negative Binomial

<!-- \begin{align*} -->
<!-- f(x) & = \left( \begin{array}{c} -->
<!-- x+j-1  \\ -->
<!-- x  \end{array} \right)(1-\pi)^x\pi^j,\ \ x,\in \{0,1,\ldots,\infty\}, 0 \leq \pi \leq 1\\ -->
<!-- \\ -->
<!-- L(\lambda|x) & = \prod_{i=1}^n f(x_i) \\ -->
<!-- \\ -->
<!-- L(\lambda|x) & = \prod_{i=1}^n \left( \begin{array}{c} -->
<!-- x_i+j-1  \\ -->
<!-- x_i  \end{array} \right)(1-\pi)^{\sum_{i=1}^n x_i}\pi^{nj}\\ -->
<!-- \\ -->
<!-- l(\lambda|x) & = \sum_{i=1}^n \ln \left( \begin{array}{c} -->
<!-- x_i+j-1  \\ -->
<!-- x_i  \end{array} \right)+{\sum_{i=1}^n x_i} \ln (1-\pi) + {nj} \ln(\pi) -->
<!-- \end{align*} -->


\begin{align*} 
p(x) & =  {\frac {\Gamma (r+x)}{x!\,\Gamma (r)}}\left({\frac {m}{r+m}}\right)^{x}\left({\frac {r}{r+m}}\right)^{r}\quad {\text{for }}x=0,1,2,\dotsc \\
\\
L(m,r|x) & = \prod_{i=1}^n p(x_i) \\
\\
L(m,r|x) & ={[\frac{1}{\Gamma (r)}]}^{n} \prod_{i=1}^{n}{\frac{\Gamma (r+x_i)}{x_{i}!}} (\frac{m}{r + m})^{\Sigma_{i=1}^n x_i} (\frac{r}{r + m})^{nr}   \\
\\
l(m,r|x) & = -n\ln[\Gamma (r)] + \sum^{n}_{i=1} \ln(\Gamma (r + x_i)) -\sum^{n}_{i=1}\ln x_i! + \sum^{n}_{i=1} x_{i} \ln (\frac{m}{r + m}) + nr \ln (\frac{r}{r + m})
\end{align*}

This parameterisation of the negative binomial is characterised by the mean parameter m and the shape parameter r.
It is important to note that the variance of a Negative Binomial under this parameterisationg is $m + \frac{m^2}{r}$. Shape parameters are often regarded as nuisance parameters and do not play a meaningful role in maximising likelihood. Therefore, since we desire no under or over dispersion, we can express the shape parameter as a function of the mean parameter to be esimated and the sample variance.

CHECK THIS! LOOK AT PAPER CALLED ESTIMATING SHAPE. I HAVE HIGHLIGHTED SOME STUFF ON SECOND PAGE

\begin{align*}
Var(x) &= m + \frac{m^2}{r} \\
r &= \frac{m^2}{Var(x) - m} \\
\hat{r} &= \frac{\hat{m^2}}{S^2 - \hat{m}}
\end{align*}

```{r negative_binomial_fit}

#https://en.wikipedia.org/wiki/Negative_binomial_distribution#Gamma%E2%80%93Poisson_mixture
#use optim to optimise for r and m
#r is a shape parameter so it doesnt really affect the loglikelihood much
# we want mean = variance i think... therefore you can solve for r = m^2 /(sample variance  - m)
#This is done by looking at birgit's notes and seeing that variance = m + m^2/r

calc_negbinom_loglik <- function(par){
  m <- par[1]
  r <- m^2 /(var(x)-m)
  
  loglik <- -n*log(gamma(r)) + sum(log(gamma(r+x))) - sum(log(factorial(x))) + sum(x) * log(m/(m+r)) + n*r*log(r/(r+m))
  
  return(loglik)
  
}


#If we maximise loglikelihood, we will be minimising AIC

mle_negbinom <-
  optim(
  par = 1,
  fn = calc_negbinom_loglik ,
  control = list(fnscale = -1),
  lower = 3,
  upper = 30 ,
  method = "L-BFGS-B"
  )

m_estimate <- mle_negbinom$par[1]
r_estimate <- m_estimate^2 /(var(x)-m_estimate)

aic_negbinom <- -2*-mle_negbinom$value + 2*(1)


data_frame(`M Mean Estimate`= m_estimate , `r Shape Estimate` = r_estimate, AIC = aic_negbinom) %>% knitr::kable()


```



###Mixture of 2 Poissons

A finite mixture distributon of two Poisson variables will now be explored. A possibly reason for the overdispersion is that the data are from two separate Poisson distributions. Since it is not known from which distribution that any given data point is from, presuming that the two distribution mixture is appropriate, an additional mixing parameter $p$ needs to be estimated.


\begin{align*} 
p(x|\lambda_1,\lambda_2,p) & =  p\frac{e^{-\lambda_1} \lambda_1^x}{x!} + (1-p)\frac{e^{-\lambda_2} \lambda_2^x}{x!},\ \ x\in \{0,1,\ldots,\infty\},\lambda_1 , \lambda_2 , p >0 \\
\\
L(\lambda_1,\lambda_2,p|x) & = \prod_{i=1}^n p(x_i) \\
\\
L(\lambda_1,\lambda_2,p|x) & =  \prod_{i=1}^n p\frac{e^{-\lambda_1} \lambda_1^x_i}{x_i!} + (1-p)\frac{e^{-\lambda_2} \lambda_2^x_i}{x_i!} \\
\\
l(\lambda_1,\lambda_2,p|x) & =-n\lambda +  \left(\sum_{i=1}^n x_i\right)\ln \lambda - \sum_{i=1}^{n}\ln(x_i!)
\end{align*}



-Likelihood 
-define all parameters
-loglikelihood

-fit to the data

-Here I am assuming the mixture will be poisson with rate = sample mean and the other poisson will have a rate of 0.1 to take into account the zero inflation ? 

###Zero inflated Poisson

-Likelihood 
-loglikelihood
-define all parameters
-fit to the data
-We can use optimisers but we must program the likelihoods ourselves

##Model Selection

-compare models and choose the best one
-Illustrate how good the model is 

-We need to reparameterize parameters so that they are unbounded

##Profile Likelihood & Confidence Intervals

-Plot likelihood surface (two parameters at a time if necessary, fixing the
other parameters at their MLEs).

-Must be program the profile likelihoods, CI's ourselves

#Results 

#Conclusion

-What are the next steps and how can we improve the models

<!-- Make title of bibliography here: -->
<!-- \newpage -->
# References  
